# COURS COMPLET : INTELLIGENCE ARTIFICIELLE
## Finance, ContrÃ´le Audit et Conseil, Marketing, Commerce International & Supply Chain Management

## Abderrahim.larhlimi@uhp.ac.ma
![](https://github.com/LarhlimiUhp/IA/blob/main/Pr%C3%A9paration/profil.png)


## MaÃ®tre de ConfÃ©rence, ENSGS
## UniversitÃ© Hassan Premier

**Niveau: 4Ã¨me annÃ©e**  
**AnnÃ©e 2025-2026**

---

## PrÃ©sentation du Manuel

Ce manuel universitaire constitue un support de cours exhaustif sur l'Intelligence Artificielle appliquÃ©e aux domaines de la Finance, du ContrÃ´le Audit et Conseil, du Marketing, du Commerce Internationalet du supply chain management.  Chaque concept, formule mathÃ©matique et algorithme est expliquÃ© en profondeur avec des exemples mÃ©tiers concrets et des cas d'entreprises rÃ©els.

### Objectifs PÃ©dagogiques

Ã€ l'issue de ce cours, vous serez capable de :

1. **Comprendre** les concepts fondamentaux de l'IA, du Machine Learning et du Deep Learning
2. **MaÃ®triser** les mÃ©thodes clÃ©s d'apprentissage supervisÃ©, non supervisÃ© et de deep learning
3. **Appliquer** l'IA Ã  des problÃ©matiques concrÃ¨tes de finance et de gestion
4. **DÃ©velopper** des solutions reproductibles en Python avec pandas, scikit-learn et Keras
5. **Ã‰valuer** les enjeux Ã©thiques, les biais et la conformitÃ© (RGPD, AI Act)
6. **Piloter** des projets IA et dialoguer efficacement avec les Ã©quipes data/IT

### PrÃ©requis

- **MathÃ©matiques** : Statistiques descriptives et infÃ©rentielles, algÃ¨bre linÃ©aire de base
- **Programmation** : Notions de base en Python
- **MÃ©tier** : ComprÃ©hension des KPI en finance et gestion, familiaritÃ© avec Excel/BI

### ModalitÃ©s d'Ã‰valuation

- **40%** : ContrÃ´le continu (QCM, mini-projets, participation)
- **30%** : Projet pratique (Ã©tude de cas, code, rapport, soutenance)
- **30%** : Examen final (thÃ©orie + Ã©tudes de cas)

---

## Table des MatiÃ¨res

1. [Chapitre 1 â€” Introduction Ã  l'Intelligence Artificielle](#chapitre-1)
2. [Chapitre 2 â€” Fondements MathÃ©matiques](#chapitre-2)
3. [Chapitre 3 â€” Apprentissage SupervisÃ©](#chapitre-3)
4. [Chapitre 4 â€” Apprentissage Non SupervisÃ©](#chapitre-4)
5. [Chapitre 5 â€” Deep Learning](#chapitre-5)
6. [Chapitre 6 â€” Natural Language Processing (NLP)](#chapitre-6)
7. [Chapitre 7 â€” IA en Finance](#chapitre-7)
8. [Chapitre 8 â€” IA en Controle Audit et Conseil](#chapitre-8)
9. [Chapitre 9 â€” IA en Marketing](#chapitre-9)
10. [Chapitre 10 â€” IA en Commerce International](#chapitre-10)
11. [Chapitre 11 â€” IA en Supply Chain Management](#chapitre-11)
12. [Chapitre 12 â€” MLOps et DÃ©ploiement](#chapitre-12)
13. [Conclusion](#Conclusion)


---

<a name="chapitre-1"></a>
# Chapitre 1 â€” Introduction Ã  l'Intelligence Artificielle

## 1.1 Qu'est-ce que l'Intelligence Artificielle ?

### 1.1.1 DÃ©finition Formelle

L'Intelligence Artificielle (IA) est une branche de l'informatique qui vise Ã  crÃ©er des systÃ¨mes capables d'effectuer des tÃ¢ches qui nÃ©cessitent traditionnellement l'intelligence humaine. Ces tÃ¢ches incluent :

- Le **raisonnement** et la rÃ©solution de problÃ¨mes complexes
- La **perception** et l'interprÃ©tation de donnÃ©es sensorielles
- La **comprÃ©hension** du langage naturel
- La **prise de dÃ©cision** dans des environnements incertains
- L'**apprentissage** Ã  partir de l'expÃ©rience

Formellement, l'IA cherche Ã  modÃ©liser et Ã  implÃ©menter des fonctions cognitives en utilisant des algorithmes et des architectures computationnelles.

### 1.1.2 DÃ©finition Intuitive

Imaginez un systÃ¨me informatique qui peut :
- **Apprendre** de ses erreurs comme un Ã©tudiant qui s'amÃ©liore aprÃ¨s chaque examen
- **S'adapter** Ã  de nouvelles situations sans Ãªtre reprogrammÃ© pour chaque cas
- **ReconnaÃ®tre** des patterns complexes que l'Å“il humain pourrait manquer
- **PrÃ©dire** des Ã©vÃ©nements futurs en analysant des donnÃ©es historiques

L'IA, c'est essentiellement donner aux machines la capacitÃ© d'imiter l'intelligence humaine, non pas en copiant le cerveau, mais en utilisant des approches mathÃ©matiques et statistiques pour arriver Ã  des rÃ©sultats similaires.

### 1.1.3 Exemple Concret

**Reconnaissance faciale sur votre smartphone** :
- Lorsque vous dÃ©verrouillez votre tÃ©lÃ©phone avec votre visage, un systÃ¨me d'IA :
  1. **DÃ©tecte** votre visage dans l'image capturÃ©e par la camÃ©ra
  2. **Extrait** des caractÃ©ristiques uniques (distance entre les yeux, forme du nez, etc.)
  3. **Compare** ces caractÃ©ristiques avec le modÃ¨le stockÃ© lors de l'enregistrement
  4. **DÃ©cide** de dÃ©verrouiller ou non en quelques millisecondes

Ce processus utilise des rÃ©seaux de neurones profonds entraÃ®nÃ©s sur des millions d'images de visages.

### 1.1.4 Exemples MÃ©tier

#### Finance : DÃ©tection de Fraude Bancaire
Les banques utilisent l'IA pour analyser en temps rÃ©el les transactions et dÃ©tecter les comportements suspects :
- **Analyse de patterns** : Une transaction de 5000â‚¬ Ã  3h du matin au Japon alors que vous vivez au Maroc
- **Score de risque** : Attribution automatique d'un score de probabilitÃ© de fraude
- **DÃ©cision automatisÃ©e** : Blocage temporaire de la carte et envoi d'une alerte SMS
- **Apprentissage continu** : Le systÃ¨me s'amÃ©liore en analysant les retours (vraie fraude vs fausse alerte)

#### Gestion : PrÃ©diction du Turnover RH
Les entreprises prÃ©disent quels employÃ©s risquent de dÃ©missionner :
- **Variables analysÃ©es** : AnciennetÃ©, salaire, promotions, satisfaction, absences
- **ModÃ¨le prÃ©dictif** : Calcul d'un score de risque de dÃ©part pour chaque employÃ©
- **Action prÃ©ventive** : Programmes de rÃ©tention ciblÃ©s sur les employÃ©s Ã  risque
- **ROI mesurable** : RÃ©duction de 20-30% du turnover dans certaines entreprises

### 1.1.5 SchÃ©ma Explicatif (Description Textuelle)

![ SYSTÃˆME D'INTELLIGENCE ARTIFICIELLE](https://github.com/LarhlimiUhp/IA/blob/main/Pr%C3%A9paration/IA.png)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SYSTÃˆME D'INTELLIGENCE ARTIFICIELLE            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DONNÃ‰ES        â”‚â”€â”€â”€>â”‚   ALGORITHMES    â”‚â”€â”€â”€>â”‚   DÃ‰CISIONS/     â”‚
â”‚   D'ENTRÃ‰E       â”‚    â”‚   D'APPRENTISSAGEâ”‚    â”‚   PRÃ‰DICTIONS    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                        â”‚
        â”‚                       â†“                        â”‚
        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
        â”‚              â”‚    MODÃˆLE        â”‚              â”‚
        â”‚              â”‚    ENTRAÃNÃ‰      â”‚              â”‚
        â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
        â”‚                       â”‚                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   FEEDBACK       â”‚
                        â”‚   ET AMÃ‰LIORATIONâ”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explication du schÃ©ma** :
1. **DonnÃ©es d'entrÃ©e** : Informations brutes (images, textes, chiffres, transactions)
2. **Algorithmes d'apprentissage** : MÃ©thodes mathÃ©matiques qui analysent les donnÃ©es
3. **ModÃ¨le entraÃ®nÃ©** : ReprÃ©sentation mathÃ©matique des patterns dÃ©couverts
4. **DÃ©cisions/PrÃ©dictions** : RÃ©sultats produits par le systÃ¨me
5. **Feedback** : Ã‰valuation des rÃ©sultats pour amÃ©liorer le modÃ¨le
6. **Boucle d'amÃ©lioration** : Le systÃ¨me s'amÃ©liore continuellement grÃ¢ce au feedback

### 1.1.6 Cas RÃ©el d'Entreprise : Netflix

**Contexte** :
Netflix compte plus de 230 millions d'abonnÃ©s dans le monde qui choisissent parmi un catalogue de plus de 15 000 titres. Sans IA, trouver un contenu intÃ©ressant serait comme chercher une aiguille dans une botte de foin.

**Solution IA Mise en Place** :
Netflix utilise un systÃ¨me de recommandation sophistiquÃ© qui combine plusieurs algorithmes d'IA :

1. **Filtrage Collaboratif** :
   - Analyse les habitudes de visionnage de millions d'utilisateurs
   - Identifie des groupes d'utilisateurs avec des goÃ»ts similaires
   - Recommande ce que des utilisateurs similaires ont aimÃ©

2. **Filtrage basÃ© sur le Contenu** :
   - Analyse les mÃ©tadonnÃ©es de chaque film/sÃ©rie (genre, acteurs, rÃ©alisateur, thÃ¨mes)
   - Identifie les caractÃ©ristiques des contenus que vous avez aimÃ©s
   - SuggÃ¨re des titres avec des caractÃ©ristiques similaires

3. **Apprentissage Profond** :
   - Analyse les images des vignettes pour prÃ©dire lesquelles vous attireront
   - Personnalise mÃªme les vignettes affichÃ©es selon vos prÃ©fÃ©rences
   - Optimise l'ordre de prÃ©sentation des recommandations

**RÃ©sultats Mesurables** :
- **80%** du contenu visionnÃ© provient des recommandations de l'IA
- **Ã‰conomie de 1 milliard de dollars** par an en rÃ©tention d'abonnÃ©s
- **Engagement accru** : Les utilisateurs passent moins de temps Ã  chercher et plus Ã  regarder
- **Personnalisation** : Chaque utilisateur voit une page d'accueil unique

**Technologies UtilisÃ©es** :
- Python, Apache Spark pour le traitement big data
- TensorFlow et PyTorch pour les modÃ¨les de deep learning
- A/B testing continu pour optimiser les algorithmes

---

## 1.2 Machine Learning (Apprentissage Automatique)

### 1.2.1 DÃ©finition Formelle

Le Machine Learning (ML) est une sous-discipline de l'IA qui se concentre sur le dÃ©veloppement d'algorithmes permettant aux ordinateurs d'apprendre Ã  partir de donnÃ©es et d'amÃ©liorer leurs performances sur une tÃ¢che spÃ©cifique sans Ãªtre explicitement programmÃ©s pour chaque cas.

Formellement, un programme informatique apprend de l'expÃ©rience E par rapport Ã  une classe de tÃ¢ches T et une mesure de performance P, si sa performance sur T, mesurÃ©e par P, s'amÃ©liore avec l'expÃ©rience E.

**Ã‰quation fondamentale du ML** :


![ SYSTÃˆME ML](https://github.com/LarhlimiUhp/IA/blob/main/Pr%C3%A9paration/prediction.png)
```
Apprendre : DonnÃ©es + Algorithme â†’ ModÃ¨le
PrÃ©dire  : Nouvelles DonnÃ©es + ModÃ¨le â†’ PrÃ©dictions
```

### 1.2.2 DÃ©finition Intuitive

Le Machine Learning, c'est comme enseigner Ã  un enfant Ã  reconnaÃ®tre des animaux :
- Au lieu de programmer des rÃ¨gles explicites ("si Ã§a a 4 pattes, un museau et aboie, c'est un chien")
- On montre des **exemples** (des milliers de photos de chiens)
- L'algorithme **dÃ©couvre lui-mÃªme** les patterns qui caractÃ©risent un chien
- Il peut ensuite **gÃ©nÃ©raliser** pour reconnaÃ®tre de nouveaux chiens jamais vus

La diffÃ©rence avec la programmation traditionnelle :
- **Programmation classique** : RÃ¨gles + DonnÃ©es â†’ RÃ©sultats
- **Machine Learning** : DonnÃ©es + RÃ©sultats â†’ RÃ¨gles (modÃ¨le)

### 1.2.3 Exemple Concret

**Filtre anti-spam d'email** :

**Approche traditionnelle (programmation classique)** :
```python
# RÃ¨gles explicites programmÃ©es manuellement
if "lottery" in email or "prince" in email:
    return "SPAM"
else:
    return "HAM" (lÃ©gitime)
```
ProblÃ¨me : Les spammeurs contournent facilement ces rÃ¨gles

**Approche Machine Learning** :
```python
# 1. Collecter des exemples
emails_spam = ["You won the lottery!", ...]
emails_ham = ["Meeting at 3pm", "Project report attached", ...]

# 2. EntraÃ®ner un modÃ¨le
model = NaiveBayes()
model.fit(emails, labels)  # Apprend automatiquement les patterns

# 3. PrÃ©dire sur de nouveaux emails
new_email = "Congratulations! Click here to claim prize"
prediction = model.predict(new_email)  # â†’ SPAM (probabilitÃ©: 95%)
```

Le modÃ¨le apprend automatiquement que :
- Certains mots sont plus frÃ©quents dans les spams ("free", "win", "click")
- La structure des phrases diffÃ¨re (beaucoup de ponctuation, MAJUSCULES)
- Les spams contiennent souvent des liens suspects

### 1.2.4 Exemples MÃ©tier

#### Finance : Scoring de CrÃ©dit AutomatisÃ©

**ProblÃ©matique** :
Une banque reÃ§oit 10 000 demandes de crÃ©dit par mois. Comment dÃ©cider rapidement et objectivement qui obtient un prÃªt ?

**Solution ML** :
```
1. DONNÃ‰ES HISTORIQUES (5 ans)
   - 100 000 dossiers de crÃ©dit passÃ©s
   - Pour chaque client : Ã¢ge, revenu, emploi, dettes, historique bancaire
   - RÃ©sultat connu : a remboursÃ© (0) ou fait dÃ©faut (1)

2. ENTRAÃNEMENT DU MODÃˆLE
   - Algorithme : RÃ©gression Logistique ou XGBoost
   - Le modÃ¨le apprend les combinaisons de facteurs qui prÃ©disent le dÃ©faut
   - Exemples de patterns dÃ©couverts :
     * Ratio dette/revenu > 40% â†’ risque Ã©levÃ©
     * Emploi stable + Ã©pargne >50k â†’ risque faible
     * Ã‚ge < 25 + crÃ©dit > 20k â†’ risque moyen-Ã©levÃ©

3. MISE EN PRODUCTION
   - Nouveau dossier â†’ Score de 0 Ã  1000
   - Score > 700 : Approbation automatique
   - Score 400-700 : Examen manuel
   - Score < 400 : Refus automatique

4. RÃ‰SULTATS
   - Temps de dÃ©cision : 48h â†’ 5 minutes
   - Taux de dÃ©faut rÃ©duit de 15%
   - ObjectivitÃ© accrue (rÃ©duction des biais humains)
```

#### Gestion : PrÃ©vision des Ventes pour Optimiser les Stocks

**ProblÃ©matique** :
Un retailer doit commander les quantitÃ©s optimales pour chaque magasin et chaque produit. Trop = invendus, Pas assez = ruptures = clients mÃ©contents.

**Solution ML** :
```
1. DONNÃ‰ES COLLECTÃ‰ES
   - Historique des ventes (3 ans, par jour, produit, magasin)
   - Facteurs externes : mÃ©tÃ©o, jours fÃ©riÃ©s, promotions, Ã©vÃ©nements
   - CaractÃ©ristiques produit : catÃ©gorie, prix, saisonnalitÃ©

2. MODÃˆLE DE PRÃ‰VISION
   - Algorithme : LSTM (rÃ©seaux de neurones rÃ©currents) ou Prophet
   - Le modÃ¨le apprend :
     * Les tendances saisonniÃ¨res (+ ventes de glaces en Ã©tÃ©)
     * L'impact des promotions (+30% lors des -20%)
     * Les corrÃ©lations entre produits (chips + sodas)

3. PRÃ‰DICTIONS QUOTIDIENNES
   - PrÃ©vision Ã  7 jours : QuantitÃ© attendue par produit/magasin
   - Intervalle de confiance : Min-Max pour gÃ©rer l'incertitude
   - Alertes automatiques si risque de rupture

4. IMPACT BUSINESS
   - RÃ©duction des ruptures de stock : -25%
   - Diminution des invendus : -18%
   - Satisfaction client : +12%
   - ROI : 300% la premiÃ¨re annÃ©e
```

### 1.2.5 SchÃ©ma Explicatif : Les Trois Types d'Apprentissage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MACHINE LEARNING                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                  â”‚                  â”‚
          â†“                  â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APPRENTISSAGE  â”‚  â”‚  APPRENTISSAGE   â”‚  â”‚  APPRENTISSAGE   â”‚
â”‚    SUPERVISÃ‰    â”‚  â”‚   NON SUPERVISÃ‰  â”‚  â”‚  PAR RENFORCEMENTâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DonnÃ©es Ã©tiquetÃ©es    DonnÃ©es non          Agent + Environnement
(X, y)               Ã©tiquetÃ©es (X)        RÃ©compenses/PÃ©nalitÃ©s

Exemples:             Exemples:            Exemples:
â€¢ Classification      â€¢ Clustering         â€¢ Jeux (Ã©checs, Go)
â€¢ RÃ©gression         â€¢ RÃ©duction dimension â€¢ Robots autonomes
â€¢ PrÃ©diction         â€¢ DÃ©tection anomalies â€¢ Trading algorithmique

Algorithmes:          Algorithmes:         Algorithmes:
â€¢ RÃ©gression linÃ©aire â€¢ K-means           â€¢ Q-Learning
â€¢ Random Forest      â€¢ PCA               â€¢ Deep Q-Network
â€¢ Neural Networks    â€¢ DBSCAN            â€¢ AlphaGo
```

**Explication dÃ©taillÃ©e des trois types** :

1. **Apprentissage SupervisÃ©** (comme un Ã©lÃ¨ve avec un professeur)
   - On fournit des **exemples avec les rÃ©ponses** (donnÃ©es Ã©tiquetÃ©es)
   - Le modÃ¨le apprend la relation entre entrÃ©es (X) et sorties (y)
   - UtilisÃ© pour : prÃ©dire, classifier, estimer

2. **Apprentissage Non SupervisÃ©** (comme un explorateur)
   - On fournit des **donnÃ©es sans rÃ©ponses** (non Ã©tiquetÃ©es)
   - Le modÃ¨le dÃ©couvre la structure cachÃ©e dans les donnÃ©es
   - UtilisÃ© pour : segmenter, rÃ©duire la dimensionnalitÃ©, dÃ©tecter des anomalies

3. **Apprentissage par Renforcement** (comme un enfant qui apprend par essai-erreur)
   - Un **agent** interagit avec un **environnement**
   - Il reÃ§oit des **rÃ©compenses** pour les bonnes actions, des **pÃ©nalitÃ©s** pour les mauvaises
   - Il apprend la **stratÃ©gie optimale** pour maximiser les rÃ©compenses cumulÃ©es

### 1.2.6 Cas RÃ©el d'Entreprise : Amazon

**SystÃ¨me de Recommandation "Customers Who Bought This Also Bought"**

**Contexte** :
Amazon vend des centaines de millions de produits. Comment suggÃ©rer les bons produits Ã  chaque client parmi cette immensitÃ© ?

**Solution ML DÃ©ployÃ©e** :

**1. Collecte de DonnÃ©es Massives**
```
Pour chaque utilisateur :
- Historique d'achats
- Produits consultÃ©s
- Temps passÃ© sur chaque page
- Produits ajoutÃ©s au panier puis retirÃ©s
- Recherches effectuÃ©es
- Avis laissÃ©s
```

**2. Algorithmes ML UtilisÃ©s**

a) **Filtrage Collaboratif (Item-to-Item)**
```python
# Pseudo-code simplifiÃ©
# Si vous achetez un livre de science-fiction:

similarities = {}
for other_item in all_items:
    users_who_bought_both = count_users(book, other_item)
    similarity_score = cosine_similarity(book, other_item)
    similarities[other_item] = similarity_score

# Recommander les items avec les scores les plus Ã©levÃ©s
top_recommendations = sorted(similarities)[:10]
```

b) **Analyse des Patterns d'Achat**
```
Pattern dÃ©couvert : 
Achat(Nintendo Switch) â†’ forte probabilitÃ© d'acheter :
- Jeux Switch (95%)
- Manette supplÃ©mentaire (70%)
- Pochette de transport (60%)
- Carte SD (55%)

â†’ Afficher ces produits dans "FrÃ©quemment achetÃ©s ensemble"
```

**3. Architecture Technique**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DONNÃ‰ES CLIENT  â”‚ â†’ Historique + Comportement temps rÃ©el
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODÃˆLES ML (plusieurs algorithmes)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Collaborative Filtering            â”‚
â”‚ â€¢ Deep Learning (embeddings)         â”‚
â”‚ â€¢ Association Rules Mining           â”‚
â”‚ â€¢ Sequential Pattern Mining          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PERSONNALISATION EN TEMPS   â”‚
â”‚  RÃ‰EL                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RECOMMANDATIONS AFFICHÃ‰ES   â”‚
â”‚  (< 100ms de latence)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**4. RÃ©sultats Business**
- **35% du chiffre d'affaires** d'Amazon provient des recommandations
- **Augmentation de 29%** du panier moyen
- **Taux de conversion** : +15% pour les utilisateurs qui cliquent sur les recommandations
- **Valeur estimÃ©e** : Plusieurs milliards de dollars de revenus additionnels par an

**5. AmÃ©liorations Continues**
Amazon utilise l'A/B testing en permanence :
- Teste diffÃ©rentes versions d'algorithmes simultanÃ©ment
- Mesure l'impact sur les KPIs (taux de clic, conversion, revenus)
- DÃ©ploie automatiquement la meilleure variante
- RÃ©entraÃ®ne les modÃ¨les quotidiennement avec les nouvelles donnÃ©es

---

## 1.3 Deep Learning (Apprentissage Profond)

### 1.3.1 DÃ©finition Formelle

Le Deep Learning est une sous-catÃ©gorie du Machine Learning basÃ©e sur des rÃ©seaux de neurones artificiels comportant **plusieurs couches cachÃ©es** (d'oÃ¹ le terme "profond"). Ces architectures sont capables d'apprendre des reprÃ©sentations hiÃ©rarchiques des donnÃ©es, passant de caractÃ©ristiques simples (bords, textures) Ã  des concepts complexes (objets, scÃ¨nes).

MathÃ©matiquement, un rÃ©seau de neurones profond est une composition de fonctions :

```
f(x) = f_L(f_{L-1}(...f_2(f_1(x; Î¸â‚); Î¸â‚‚)...; Î¸_{L-1}); Î¸_L)
```

OÃ¹ :
- `x` = donnÃ©es d'entrÃ©e
- `f_i` = fonction de la couche i (transformation linÃ©aire + activation non-linÃ©aire)
- `Î¸_i` = paramÃ¨tres (poids et biais) de la couche i
- `L` = nombre de couches

### 1.3.2 DÃ©finition Intuitive

Imaginez que vous voulez reconnaÃ®tre un chat dans une image :

**Approche ML Classique** :
- Vous devez dÃ©finir manuellement les caractÃ©ristiques pertinentes
- "Extrayez la forme des oreilles, comptez les moustaches, mesurez la taille des yeux..."
- C'est fastidieux et vous pourriez manquer des caractÃ©ristiques importantes

**Approche Deep Learning** :
- Vous montrez 100 000 images de chats au rÃ©seau
- **Couche 1** apprend automatiquement les bords et les coins
- **Couche 2** combine les bords pour dÃ©tecter des textures (poils, motifs)
- **Couche 3** assemble les textures en parties (oreilles, yeux, museau)
- **Couche 4** combine les parties pour reconnaÃ®tre "un chat"
- Tout cela **sans que vous ayez Ã  programmer explicitement** ces Ã©tapes !

Analogie : C'est comme si un enfant apprenait Ã  reconnaÃ®tre des concepts de plus en plus abstraits :
- Niveau 1 : Lignes et formes
- Niveau 2 : Combinaisons de formes
- Niveau 3 : Parties d'objets
- Niveau 4 : Objets complets
- Niveau 5 : ScÃ¨nes et contextes

### 1.3.3 Exemple Concret

**Reconnaissance d'Ã©criture manuscrite (MNIST)**

Imaginons que vous voulez crÃ©er un systÃ¨me qui lit automatiquement les chiffres Ã©crits Ã  la main (0-9) :

**DonnÃ©es** :
- 60 000 images de chiffres manuscrits (28Ã—28 pixels en niveaux de gris)
- Chaque pixel a une valeur de 0 (blanc) Ã  255 (noir)

**Architecture du RÃ©seau de Neurones** :
```
INPUT â†’ CONV1 â†’ POOL1 â†’ CONV2 â†’ POOL2 â†’ FC1 â†’ FC2 â†’ OUTPUT

784 pixels     128        64        32       128      10
(28Ã—28)      neurones  neurones  neurones neurones classes
                                                    (0-9)
```

**Ce que chaque couche apprend** :
1. **CONV1** (Convolution 1) : DÃ©tecte des bords simples
   - Filtres qui reconnaissent lignes verticales, horizontales, diagonales
   
2. **POOL1** (Pooling 1) : RÃ©duit la taille en gardant l'essentiel
   - Conserve les informations importantes, ignore les dÃ©tails
   
3. **CONV2** (Convolution 2) : DÃ©tecte des formes plus complexes
   - Coins, courbes, boucles caractÃ©ristiques des chiffres
   
4. **POOL2** (Pooling 2) : Nouvelle rÃ©duction dimensionnelle
   
5. **FC1** (Fully Connected 1) : Combine les caractÃ©ristiques
   - Apprend les combinaisons qui distinguent les chiffres
   
6. **FC2** (Fully Connected 2) : DÃ©cision finale
   - Produit 10 probabilitÃ©s (une par chiffre)
   - Ex : [0.01, 0.02, 0.85, 0.05, 0.01, 0.02, 0.01, 0.01, 0.01, 0.01]
   - â†’ Le chiffre est probablement un "2" (85% de confiance)

**RÃ©sultats** :
- PrÃ©cision : 99.2% sur le jeu de test
- Temps d'infÃ©rence : < 1ms par image
- Applications : Lecture automatique de chÃ¨ques, codes postaux, formulaires

### 1.3.4 Exemples MÃ©tier

#### Finance : PrÃ©diction de SÃ©ries Temporelles FinanciÃ¨res

**ProblÃ©matique** :
PrÃ©dire les mouvements de prix d'actions en analysant des annÃ©es de donnÃ©es historiques complexes (prix, volumes, indicateurs techniques, news, sentiment sur rÃ©seaux sociaux).

**Solution Deep Learning : LSTM (Long Short-Term Memory)**

```python
# Architecture LSTM pour prÃ©dire le prix de demain

DonnÃ©es d'entrÃ©e (sÃ©quence de 60 jours) :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Jour 1  Jour 2  ...  Jour 59  Jour 60   â”‚
â”‚ [Prix, Volume, RSI, MACD, Sentiment]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Couche LSTM (128)      â”‚ â† MÃ©morise patterns Ã  long terme
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Couche LSTM (64)       â”‚ â† Raffine les patterns
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Couche Dense (32)      â”‚ â† Combine les informations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sortie (1)             â”‚ â† Prix prÃ©dit pour demain
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ce que le modÃ¨le apprend** :
- **Patterns temporels** : "AprÃ¨s une sÃ©quence de 5 jours de hausse modÃ©rÃ©e, correction probable"
- **CorrÃ©lations complexes** : "Volume Ã©levÃ© + sentiment nÃ©gatif â†’ baisse imminente"
- **SaisonnalitÃ©** : "Historiquement, baisse en septembre (fiscal year-end)"
- **RÃ©action aux news** : "Annonce de bÃ©nÃ©fices supÃ©rieurs â†’ hausse dans les 3 jours suivants"

**Performances** :
- RMSE (erreur) : 2.3% sur donnÃ©es de test
- Direction correcte prÃ©dite : 62% du temps (vs 50% au hasard)
- **Attention** : Pas de "boule de cristal" ! Les marchÃ©s restent imprÃ©visibles
- Usage : Aide Ã  la dÃ©cision, pas trading automatique sans supervision

**Mise en garde** :
Les marchÃ©s financiers sont influencÃ©s par des facteurs externes imprÃ©visibles (crises, guerres, dÃ©cisions politiques). Le DL amÃ©liore les prÃ©dictions mais ne garantit jamais la performance.

#### Gestion : Analyse de Sentiment Client Ã  partir d'Avis

**ProblÃ©matique** :
Une entreprise e-commerce reÃ§oit 10 000 avis clients par jour. Comment identifier rapidement les problÃ¨mes et les opportunitÃ©s d'amÃ©lioration ?

**Solution Deep Learning : Transformers (BERT fine-tunÃ©)**

```
INPUT : Texte de l'avis client
"Le produit est arrivÃ© rapidement mais la qualitÃ© est dÃ©cevante. 
Le service client a Ã©tÃ© excellent pour gÃ©rer le retour."

         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOKENIZATION                      â”‚
â”‚  [Le, produit, est, arrivÃ©, ...]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EMBEDDINGS (contextuels)          â”‚
â”‚  Chaque mot â†’ vecteur de 768 dim   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BERT ENCODER (12 couches)         â”‚
â”‚  Comprend contexte et nuances      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLASSIFICATION HEAD               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
OUTPUT :
â€¢ Sentiment global : MITIGÃ‰ (score: 0.52)
â€¢ Aspects identifiÃ©s :
  - Livraison : POSITIF (0.89)
  - QualitÃ© produit : NÃ‰GATIF (0.15)
  - Service client : POSITIF (0.93)
```

**Ce que le modÃ¨le comprend** :
- **Nuances** : "rapidement" (positif) et "dÃ©cevante" (nÃ©gatif) dans mÃªme phrase
- **Aspects multiples** : Distingue qualitÃ© produit vs service client
- **Contexte** : "excellent" s'applique au service, pas au produit
- **Sarcasme** (limitÃ©) : DÃ©tecte certaines formes d'ironie

**RÃ©sultats Business** :
- **Automatisation** : Classification de 95% des avis sans intervention humaine
- **Alertes automatiques** : Email au chef de produit si >10 avis nÃ©gatifs sur qualitÃ© en 24h
- **Dashboard temps rÃ©el** : Sentiment par catÃ©gorie de produit, Ã©volution temporelle
- **ROI** : Ã‰conomie de 50 000â‚¬/an en analyse manuelle + amÃ©lioration produits

### 1.3.5 SchÃ©ma Explicatif : Architectures Deep Learning par Type de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEEP LEARNING ARCHITECTURES                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   IMAGES / VISION   â”‚  â”‚  SÃ‰QUENCES / TEMPS   â”‚  â”‚    TEXTE / NLP      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                        â”‚                         â”‚
          â†“                        â†“                         â†“
    CNN (RÃ©seaux de          RNN / LSTM / GRU          Transformers
     Convolution)           (RÃ©seaux RÃ©currents)        (Attention)
          â”‚                        â”‚                         â”‚
          â†“                        â†“                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COUCHES CONV        â”‚  â”‚ CELLULES MÃ‰MOIRE     â”‚  â”‚ SELF-ATTENTION      â”‚
â”‚ â€¢ Filtres dÃ©tectent â”‚  â”‚ â€¢ MÃ©morisent passÃ©   â”‚  â”‚ â€¢ Relations entre   â”‚
â”‚   patterns locaux   â”‚  â”‚ â€¢ GÃ¨rent dÃ©pendances â”‚  â”‚   mots              â”‚
â”‚ â€¢ HiÃ©rarchie        â”‚  â”‚   temporelles        â”‚  â”‚ â€¢ ParallÃ©lisation   â”‚
â”‚   caractÃ©ristiques  â”‚  â”‚                      â”‚  â”‚   efficace          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                        â”‚                         â”‚
          â†“                        â†“                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       APPLICATIONS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Reconnaissance â”‚ â€¢ PrÃ©diction prix  â”‚ â€¢ Traduction           â”‚
â”‚   faciale        â”‚ â€¢ DÃ©tection fraude â”‚ â€¢ Chatbots             â”‚
â”‚ â€¢ Diagnostic     â”‚   temporelle       â”‚ â€¢ Analyse sentiment    â”‚
â”‚   mÃ©dical        â”‚ â€¢ PrÃ©vision ventes â”‚ â€¢ RÃ©sumÃ© texte         â”‚
â”‚ â€¢ Conduite auto  â”‚ â€¢ Trading algo     â”‚ â€¢ Q&A                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Explication des architectures** :

1. **CNN (Convolutional Neural Networks)** - Pour les images
   - **Principe** : Filtres glissants qui dÃ©tectent des patterns locaux
   - **Couches** : Convolution â†’ Pooling â†’ Convolution â†’ Pooling â†’ Dense
   - **Force** : Invariance Ã  la translation (dÃ©tecte un chat peu importe sa position)
   - **Exemple** : ResNet (152 couches), utilisÃ© par Facebook pour taguer automatiquement les photos

2. **RNN/LSTM** - Pour les sÃ©quences temporelles
   - **Principe** : Cellules avec mÃ©moire qui traitent les donnÃ©es sÃ©quentiellement
   - **LSTM** : AmÃ©lioration du RNN, gÃ¨re mieux les dÃ©pendances Ã  long terme
   - **Force** : Comprend le contexte temporel
   - **Exemple** : Google Translate (avant Transformers)

3. **Transformers** - Pour le texte et au-delÃ 
   - **Principe** : MÃ©canisme d'attention pour capter les relations entre tous les mots
   - **Force** : ParallÃ©lisable (plus rapide), gÃ¨re mieux les longues sÃ©quences
   - **Exemples** : BERT (Google), GPT (OpenAI), LLaMA (Meta)

### 1.3.6 Cas RÃ©el d'Entreprise : Tesla - Autopilot

**Contexte** :
Tesla dÃ©veloppe un systÃ¨me de conduite autonome qui doit comprendre l'environnement routier en temps rÃ©el Ã  partir de 8 camÃ©ras montÃ©es sur le vÃ©hicule.

**DÃ©fi Technique** :
- Traiter 8 flux vidÃ©o simultanÃ©ment (1280Ã—960 pixels, 36 FPS)
- Identifier : voitures, piÃ©tons, cyclistes, panneaux, marquages au sol, feux de signalisation
- PrÃ©dire les trajectoires des autres vÃ©hicules
- Prendre des dÃ©cisions en moins de 100ms
- Fonctionner dans toutes les conditions (jour, nuit, pluie, neige)

**Architecture Deep Learning** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              8 CAMÃ‰RAS â†’ FLUX VIDÃ‰O EN TEMPS RÃ‰EL            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          RÃ‰SEAU DE NEURONES CONVOLUTIONNEL (CNN)             â”‚
â”‚                      HydraNet Architecture                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BACKBONE (tronc commun) - ResNet modifiÃ©                    â”‚
â”‚  â€¢ 50 couches de convolution                                 â”‚
â”‚  â€¢ Extrait caractÃ©ristiques visuelles                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚         â”‚             â”‚              â”‚
    â†“         â†“         â†“             â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚DÃ‰TECTIONâ”‚ â”‚SEGMENâ”‚ â”‚ ESTIMA- â”‚ â”‚PRÃ‰DICTIONâ”‚ â”‚ DÃ‰TECTION   â”‚
â”‚OBJETS   â”‚ â”‚TATIONâ”‚ â”‚ TION    â”‚ â”‚TRAJECTO  â”‚ â”‚ PROFONDEUR  â”‚
â”‚(Boundingâ”‚ â”‚SÃ‰MAN-â”‚ â”‚ PROFOND.â”‚ â”‚-IRES     â”‚ â”‚ (Distance)  â”‚
â”‚Boxes)   â”‚ â”‚TIQUE â”‚ â”‚         â”‚ â”‚          â”‚ â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚         â”‚             â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  MODULE DE FUSION    â”‚
              â”‚  Combine toutes les  â”‚
              â”‚  informations        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  PLANIFICATION       â”‚
              â”‚  DE TRAJECTOIRE      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  COMMANDES VÃ‰HICULE  â”‚
              â”‚  AccÃ©lÃ©ration        â”‚
              â”‚  Freinage            â”‚
              â”‚  Direction           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DonnÃ©es d'EntraÃ®nement** :
- **10 milliards de miles** parcourus par la flotte Tesla
- **1 milliard d'images** annotÃ©es (objets, distances, trajectoires)
- **Annotation semi-automatique** : Les modÃ¨les actuels aident Ã  annoter de nouvelles donnÃ©es
- **Shadow Mode** : Les nouvelles versions tournent en arriÃ¨re-plan pour collecter des donnÃ©es sans contrÃ´ler le vÃ©hicule

**EntraÃ®nement du ModÃ¨le** :
```
MatÃ©riel :
- 10 000 GPUs NVIDIA A100
- Supercalculateur Dojo (conÃ§u par Tesla)
- 1 Exaflop de puissance de calcul

Temps d'entraÃ®nement :
- 2-3 semaines pour une version majeure
- RÃ©entraÃ®nement continu avec nouvelles donnÃ©es

Optimisations :
- Quantification (rÃ©duction prÃ©cision : FP32 â†’ INT8)
- Pruning (suppression neurones inutiles)
- Distillation (transfert vers modÃ¨le plus petit)
â†’ Objectif : Tenir sur le hardware embarquÃ© (chip FSD)
```

**DÃ©ploiement** :
```
Tesla FSD Chip (Hardware 3.0) :
- 144 TOPS (Trillions Operations Per Second)
- Consommation : 72 Watts
- Redondance : 2 chips indÃ©pendants (sÃ©curitÃ©)
- Mise Ã  jour OTA (Over-The-Air) du modÃ¨le

Pipeline d'infÃ©rence :
Images (8 camÃ©ras) â†’ Preprocessing â†’ ModÃ¨le CNN
                                       â†“
                                  DÃ©tections
                                       â†“
                                  Fusion sensorielle
                                       â†“
                                  DÃ©cisions
                                       â†“
                        Commandes vÃ©hicule (< 100ms)
```

**RÃ©sultats et DÃ©fis** :

âœ… **SuccÃ¨s** :
- **10 fois moins d'accidents** qu'un conducteur humain moyen (selon Tesla)
- **AmÃ©lioration continue** : Chaque version (FSD 11, 12, 13...) plus performante
- **GÃ©nÃ©ralisation** : Fonctionne dans des scenarios jamais vus en entraÃ®nement

âš  **DÃ©fis Restants** :
- **Edge cases** : Situations rares (cÃ´ne de chantier inhabituel, main d'un policier)
- **Conditions extrÃªmes** : Neige Ã©paisse, brouillard dense
- **ResponsabilitÃ© lÃ©gale** : Qui est responsable en cas d'accident ?
- **Biais gÃ©ographiques** : Performant aux USA, moins en Europe/Asie

**Enseignements pour l'IA en Entreprise** :
1. **DonnÃ©es = Carburant** : Plus de donnÃ©es â†’ meilleur modÃ¨le
2. **ItÃ©ration continue** : DÃ©ployer, collecter feedback, amÃ©liorer
3. **Infrastructu
---

<a name="chapitre-8-finance"></a>
# Chapitre 8 â€” IA en Finance

## 8.1 Introduction Ã  l'IA en Finance

### 8.1.1 Vue d'Ensemble du Secteur

Le secteur financier est l'un des plus grands adopteurs de l'Intelligence Artificielle, avec des investissements dÃ©passant les 35 milliards de dollars en 2024.

**Domaines d'Application Principaux** :
- **Trading Algorithmique** : ExÃ©cution automatisÃ©e d'ordres boursiers
- **Scoring de CrÃ©dit** : Ã‰valuation automatique de la solvabilitÃ©
- **DÃ©tection de Fraude** : Identification de transactions suspectes en temps rÃ©el
- **Robo-Advisors** : Conseils d'investissement personnalisÃ©s automatisÃ©s
- **ConformitÃ© (KYC/AML)** : Automatisation des processus rÃ©glementaires

**ROI Typiques** :
- 35-40% de rÃ©duction des coÃ»ts opÃ©rationnels
- 25-30% d'amÃ©lioration de la dÃ©tection de fraude
- 50-60% de rÃ©duction du temps de traitement des crÃ©dits

---

## 8.2 Cas d'Usage : Scoring de CrÃ©dit Intelligent

### 8.2.1 ProblÃ©matique

Les banques doivent Ã©valuer rapidement la solvabilitÃ© de milliers de demandeurs tout en :
- Minimisant les pertes (dÃ©fauts de paiement)
- Maximisant l'inclusion financiÃ¨re
- Respectant les rÃ©glementations (RGPD, Ã©quitÃ©)

### 8.2.2 Solution IA

**Variables UtilisÃ©es** (100+) :
- Traditionnelles : revenus, dettes, historique crÃ©dit
- Alternatives : comportement transactionnel, digital footprint
- DÃ©rivÃ©es : ratios financiers, scores de stabilitÃ©

**ModÃ¨les** :
- XGBoost / LightGBM pour la performance
- Random Forest pour l'interprÃ©tabilitÃ©
- Ensemble voting pour la robustesse

**MÃ©triques ClÃ©s** :
- AUC-ROC > 0.80 (capacitÃ© de discrimination)
- Gini Coefficient > 0.50
- KS Statistic > 0.40
- Brier Score < 0.15 (calibration)

---

## 8.3 Cas d'Usage : DÃ©tection de Fraude en Temps RÃ©el

### 8.3.1 Types de Fraudes

1. **Fraude Ã  la carte** : Transactions non autorisÃ©es
2. **Prise de contrÃ´le de compte** : Vol de credentials
3. **Blanchiment d'argent** : Structuring, layering
4. **Fraude Ã  l'identitÃ©** : IdentitÃ©s synthÃ©tiques

### 8.3.2 Architecture SystÃ¨me

**Pipeline Temps RÃ©el (< 100ms)** :

```
Transaction â†’ Feature Engineering â†’ ML Scoring â†’ DÃ©cision
             (Redis cache)         (XGBoost +    (Approve/
                                    Isolation    Review/
                                    Forest)      Block)
```

**Features Temps RÃ©el** :
- Historique 24h/1h de la carte
- VÃ©locitÃ© (transactions/heure)
- DÃ©viations par rapport au comportement habituel
- GÃ©olocalisation et distance parcourue
- Profil marchand (taux de fraude)

**Cas RÃ©el : PayPal**
- 17 milliards de transactions/an
- Taux de dÃ©tection : 97%
- Faux positifs : -50%
- Temps de traitement : < 100ms
- Ã‰conomies : $700M/an

---

<a name="chapitre-9-audit"></a>
# Chapitre 9 â€” IA en ContrÃ´le, Audit et Conseil

## 9.1 Introduction au ContrÃ´le et Audit AugmentÃ© par l'IA

### 9.1.1 Transformation du MÃ©tier

Le mÃ©tier de l'audit et du contrÃ´le connaÃ®t une rÃ©volution majeure grÃ¢ce Ã  l'IA :

**Avant l'IA** :
- Audit par Ã©chantillonnage (5-10% des transactions)
- Processus manuels et chronophages
- DÃ©tection a posteriori des anomalies
- CoÃ»t Ã©levÃ© des missions

**Avec l'IA** :
- Audit exhaustif (100% des donnÃ©es)
- Automatisation des contrÃ´les rÃ©pÃ©titifs
- DÃ©tection proactive et prÃ©dictive
- Focus auditeur sur analyse Ã  haute valeur

**Domaines d'Application** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        IA EN CONTRÃ”LE ET AUDIT                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ DÃ©tection d'anomalies comptables            â”‚
â”‚ â€¢ Analyse de conformitÃ© automatisÃ©e           â”‚
â”‚ â€¢ PrÃ©diction des risques                      â”‚
â”‚ â€¢ Automatisation de la revue documentaire     â”‚
â”‚ â€¢ Scoring de risque fournisseurs              â”‚
â”‚ â€¢ DÃ©tection de corruption et fraude interne   â”‚
â”‚ â€¢ Optimisation des processus de contrÃ´le      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9.2 DÃ©tection d'Anomalies Comptables

### 9.2.1 Loi de Benford et DÃ©tection de Fraude

**Principe** :
La Loi de Benford stipule que dans de nombreux ensembles de donnÃ©es naturelles, le premier chiffre suit une distribution logarithmique spÃ©cifique.

**Application** :
DÃ©tecter les manipulations comptables en comparant la distribution rÃ©elle vs attendue.

**ImplÃ©mentation** :

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

class BenfordAnomalyDetector:
    """
    DÃ©tection d'anomalies comptables via Loi de Benford
    """
    
    def __init__(self):
        # Distribution thÃ©orique de Benford (premier chiffre)
        self.benford_distribution = {
            1: 0.301,
            2: 0.176,
            3: 0.125,
            4: 0.097,
            5: 0.079,
            6: 0.067,
            7: 0.058,
            8: 0.051,
            9: 0.046
        }
    
    def get_first_digit(self, number):
        """Extraire le premier chiffre significatif"""
        str_num = str(abs(number)).replace('.', '').lstrip('0')
        if str_num:
            return int(str_num[0])
        return None
    
    def analyze_dataset(self, amounts, dataset_name="Dataset"):
        """
        Analyser un ensemble de montants
        """
        # Extraire premiers chiffres
        first_digits = [self.get_first_digit(amt) for amt in amounts]
        first_digits = [d for d in first_digits if d is not None]
        
        # Calculer distribution observÃ©e
        observed_dist = {}
        total = len(first_digits)
        for digit in range(1, 10):
            count = first_digits.count(digit)
            observed_dist[digit] = count / total if total > 0 else 0
        
        # Test Chi-Square
        expected_counts = [self.benford_distribution[d] * total for d in range(1, 10)]
        observed_counts = [first_digits.count(d) for d in range(1, 10)]
        
        chi2_stat, p_value = stats.chisquare(
            f_obs=observed_counts,
            f_exp=expected_counts
        )
        
        # InterprÃ©tation
        compliant = p_value > 0.05
        
        print(f"\n{'='*70}")
        print(f"ANALYSE LOI DE BENFORD - {dataset_name}")
        print(f"{'='*70}")
        print(f"\nğŸ“Š Ã‰chantillon : {total:,} transactions")
        print(f"\nğŸ”¬ Test Chi-Square")
        print(f"   Statistique : {chi2_stat:.4f}")
        print(f"   P-value     : {p_value:.4f}")
        print(f"   RÃ©sultat    : {'âœ… CONFORME' if compliant else 'ğŸ”´ SUSPECT'}")
        
        if not compliant:
            print(f"\nâš ï¸  ALERTE : Distribution non conforme Ã  la loi de Benford")
            print(f"   â†’ Possibles manipulations comptables")
            print(f"   â†’ Investigation approfondie recommandÃ©e")
        
        # Afficher Ã©carts par chiffre
        print(f"\nğŸ“ˆ Distribution par Premier Chiffre :\n")
        print(f"{'Chiffre':^10} | {'Benford':^10} | {'ObservÃ©':^10} | {'Ã‰cart':^10}")
        print(f"{'-'*50}")
        
        for digit in range(1, 10):
            benford_pct = self.benford_distribution[digit] * 100
            observed_pct = observed_dist[digit] * 100
            deviation = observed_pct - benford_pct
            
            flag = "ğŸ”´" if abs(deviation) > 5 else ""  # Ã‰cart > 5%
            print(f"{digit:^10} | {benford_pct:>9.1f}% | {observed_pct:>9.1f}% | {deviation:>+8.1f}% {flag}")
        
        return {
            'chi2_statistic': chi2_stat,
            'p_value': p_value,
            'compliant': compliant,
            'observed_distribution': observed_dist
        }

# EXEMPLE D'UTILISATION
# =====================

# Dataset 1 : DÃ©penses normales (conforme Ã  Benford)
np.random.seed(42)
normal_expenses = np.random.lognormal(mean=7, sigma=1.5, size=1000)

# Dataset 2 : DÃ©penses manipulÃ©es (anomalies)
# Fraudeur arrondit souvent Ã  100, 200, 500, 1000
manipulated_expenses = np.concatenate([
    np.random.lognormal(mean=7, sigma=1.5, size=700),  # 70% normales
    np.array([100] * 50),   # Arrondis Ã  100
    np.array([200] * 50),   # Arrondis Ã  200
    np.array([500] * 100),  # Arrondis Ã  500
    np.array([1000] * 100)  # Arrondis Ã  1000
])

detector = BenfordAnomalyDetector()

# Analyser dataset normal
results_normal = detector.analyze_dataset(normal_expenses, "DÃ©penses Normales")

# Analyser dataset manipulÃ©
results_fraud = detector.analyze_dataset(manipulated_expenses, "DÃ©penses Suspectes")
```

### 9.2.2 Machine Learning pour Anomalies Complexes

**Au-delÃ  de Benford** :
Certaines fraudes sophistiquÃ©es respectent Benford mais prÃ©sentent d'autres patterns anormaux.

```python
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import pandas as pd

class AdvancedAuditAnomalyDetector:
    """
    DÃ©tection d'anomalies multi-dimensionnelles
    """
    
    def __init__(self, contamination=0.05):
        self.contamination = contamination  # % attendu d'anomalies
        self.model = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=200
        )
        self.scaler = StandardScaler()
        
    def engineer_features(self, transactions_df):
        """
        CrÃ©er features pour dÃ©tection d'anomalies
        """
        df = transactions_df.copy()
        
        # FEATURES TEMPORELLES
        df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
        df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        df['is_night'] = ((df['hour'] < 6) | (df['hour'] > 22)).astype(int)
        df['month'] = pd.to_datetime(df['timestamp']).dt.month
        
        # FEATURES MONTANT
        df['amount_log'] = np.log1p(df['amount'])
        df['amount_rounded'] = (df['amount'] % 100 == 0).astype(int)
        df['amount_ends_in_99'] = (df['amount'] % 100 == 99).astype(int)
        
        # FEATURES COMPTE
        account_stats = df.groupby('account_id')['amount'].agg(['mean', 'std', 'count'])
        df = df.merge(
            account_stats.add_prefix('account_'),
            left_on='account_id',
            right_index=True
        )
        df['amount_vs_account_avg'] = df['amount'] / (df['account_mean'] + 1)
        
        # FEATURES BÃ‰NÃ‰FICIAIRE
        df['beneficiary_is_new'] = df['beneficiary_id'].map(
            df.groupby('beneficiary_id')['timestamp'].transform('min') == df['timestamp']
        ).astype(int)
        
        beneficiary_risk = df.groupby('beneficiary_id').size()
        df['beneficiary_frequency'] = df['beneficiary_id'].map(beneficiary_risk)
        
        # FEATURES DESCRIPTION
        df['description_length'] = df['description'].str.len()
        df['description_has_numbers'] = df['description'].str.contains(r'\d').astype(int)
        df['description_all_caps'] = (df['description'] == df['description'].str.upper()).astype(int)
        
        # FEATURES PATTERNS SUSPECTS
        df['split_payment_flag'] = (
            (df['amount'] > 9000) & (df['amount'] < 10000)
        ).astype(int)  # Juste sous seuil dÃ©claration
        
        return df
    
    def detect_anomalies(self, transactions_df):
        """
        DÃ©tecter transactions anormales
        """
        # Feature engineering
        df_featured = self.engineer_features(transactions_df)
        
        # SÃ©lectionner features numÃ©riques
        feature_cols = [
            'amount', 'amount_log', 'hour', 'day_of_week', 'is_weekend', 'is_night',
            'amount_rounded', 'amount_ends_in_99', 'account_count', 'amount_vs_account_avg',
            'beneficiary_is_new', 'beneficiary_frequency', 'description_length',
            'description_has_numbers', 'split_payment_flag'
        ]
        
        X = df_featured[feature_cols].fillna(0)
        
        # Normalisation
        X_scaled = self.scaler.fit_transform(X)
        
        # DÃ©tection anomalies
        predictions = self.model.fit_predict(X_scaled)
        scores = self.model.score_samples(X_scaled)
        
        # -1 = anomalie, 1 = normal
        df_featured['anomaly'] = predictions
        df_featured['anomaly_score'] = scores
        
        # Trier par score (plus nÃ©gatif = plus anormal)
        anomalies = df_featured[df_featured['anomaly'] == -1].sort_values('anomaly_score')
        
        print(f"\n{'='*70}")
        print(f"DÃ‰TECTION D'ANOMALIES - ISOLATION FOREST")
        print(f"{'='*70}")
        print(f"\nğŸ“Š Total transactions   : {len(df_featured):,}")
        print(f"ğŸ”´ Anomalies dÃ©tectÃ©es  : {len(anomalies):,} ({len(anomalies)/len(df_featured)*100:.2f}%)")
        print(f"ğŸ’° Montant total suspect: {anomalies['amount'].sum():,.2f} â‚¬")
        
        print(f"\nğŸ” Top 10 Transactions les Plus Suspectes :\n")
        
        for idx, (_, row) in enumerate(anomalies.head(10).iterrows(), 1):
            print(f"{idx}. Transaction #{row['transaction_id']}")
            print(f"   Montant        : {row['amount']:,.2f} â‚¬")
            print(f"   Compte         : {row['account_id']}")
            print(f"   BÃ©nÃ©ficiaire   : {row['beneficiary_id']}")
            print(f"   Date/Heure     : {row['timestamp']}")
            print(f"   Score Anomalie : {row['anomaly_score']:.4f}")
            print(f"   Raisons probables:")
            
            reasons = []
            if row['is_night']:
                reasons.append("Transaction nocturne")
            if row['amount_rounded']:
                reasons.append("Montant arrondi (suspect)")
            if row['beneficiary_is_new']:
                reasons.append("Nouveau bÃ©nÃ©ficiaire")
            if row['split_payment_flag']:
                reasons.append("Montant juste sous seuil 10kâ‚¬")
            if row['amount_vs_account_avg'] > 5:
                reasons.append(f"Montant {row['amount_vs_account_avg']:.1f}x supÃ©rieur Ã  la moyenne du compte")
            
            for reason in reasons:
                print(f"      â€¢ {reason}")
            print()
        
        return anomalies

# Exemple d'utilisation
transactions = pd.DataFrame({
    'transaction_id': range(1000),
    'timestamp': pd.date_range('2024-01-01', periods=1000, freq='H'),
    'account_id': np.random.choice(['ACC001', 'ACC002', 'ACC003'], 1000),
    'beneficiary_id': np.random.choice([f'BEN{i:03d}' for i in range(50)], 1000),
    'amount': np.concatenate([
        np.random.lognormal(7, 1, 900),  # Transactions normales
        [9950] * 50,  # Suspect: juste sous 10k
        [500] * 50    # Suspect: arrondis exacts
    ]),
    'description': ['Payment' + str(i) for i in range(1000)]
})

detector = AdvancedAuditAnomalyDetector(contamination=0.10)
anomalies_detected = detector.detect_anomalies(transactions)
```

---

## 9.3 Automatisation de la Revue Documentaire

### 9.3.1 Extraction et Classification de Documents

**ProblÃ©matique** :
Les auditeurs doivent passer en revue des milliers de documents (factures, contrats, relevÃ©s) pour vÃ©rifier conformitÃ© et dÃ©tecter anomalies.

**Solution IA** :

```python
from transformers import pipeline
import pytesseract
from PIL import Image
import pdf2image

class DocumentReviewAutomation:
    """
    Automatisation de la revue documentaire avec NLP
    """
    
    def __init__(self):
        # ModÃ¨le NLP pour classification
        self.classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli"
        )
        
        # CatÃ©gories de documents
        self.document_categories = [
            "Facture",
            "Contrat",
            "RelevÃ© bancaire",
            "Note de frais",
            "Bon de commande",
            "Rapport financier",
            "Justificatif"
        ]
        
    def extract_text_from_pdf(self, pdf_path):
        """
        Extraire texte d'un PDF (avec OCR si nÃ©cessaire)
        """
        try:
            # Convertir PDF en images
            images = pdf2image.convert_from_path(pdf_path)
            
            # OCR sur chaque page
            full_text = ""
            for img in images:
                text = pytesseract.image_to_string(img, lang='fra')
                full_text += text + "\n"
            
            return full_text
        except Exception as e:
            print(f"Erreur extraction : {e}")
            return ""
    
    def classify_document(self, text):
        """
        Classifier le type de document
        """
        result = self.classifier(
            text[:500],  # Premier 500 caractÃ¨res
            self.document_categories
        )
        
        return {
            'category': result['labels'][0],
            'confidence': result['scores'][0]
        }
    
    def extract_key_info(self, text, doc_type):
        """
        Extraire informations clÃ©s selon type de document
        """
        import re
        
        info = {}
        
        if doc_type == "Facture":
            # NumÃ©ro de facture
            facture_match = re.search(r'(?:Facture|Invoice)\s*(?:NÂ°|#|No\.?)\s*:?\s*(\w+)', text, re.I)
            if facture_match:
                info['numero_facture'] = facture_match.group(1)
            
            # Montant TTC
            montant_match = re.search(r'(?:Total|Montant)\s*TTC\s*:?\s*([\d\s,\.]+)\s*â‚¬', text, re.I)
            if montant_match:
                montant_str = montant_match.group(1).replace(' ', '').replace(',', '.')
                info['montant_ttc'] = float(montant_str)
            
            # Date
            date_match = re.search(r'(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})', text)
            if date_match:
                info['date'] = date_match.group(1)
        
        elif doc_type == "Contrat":
            # Parties contractuelles
            parties_match = re.findall(r'(?:Entre|Party)\s+(.+?)(?:\set\s|\sand\s)', text, re.I)
            if parties_match:
                info['parties'] = parties_match
            
            # Montant contractuel
            montant_match = re.search(r'(?:montant|amount)\s+(?:de|of)\s+([\d\s,\.]+)', text, re.I)
            if montant_match:
                info['montant'] = montant_match.group(1)
        
        return info
    
    def check_compliance(self, doc_info, doc_type):
        """
        VÃ©rifier conformitÃ© du document
        """
        issues = []
        
        if doc_type == "Facture":
            # VÃ©rifications obligatoires
            if 'numero_facture' not in doc_info:
                issues.append({
                    'severity': 'HIGH',
                    'issue': 'NumÃ©ro de facture manquant'
                })
            
            if 'montant_ttc' not in doc_info:
                issues.append({
                    'severity': 'HIGH',
                    'issue': 'Montant TTC non identifiÃ©'
                })
            
            if 'date' not in doc_info:
                issues.append({
                    'severity': 'MEDIUM',
                    'issue': 'Date de facture manquante'
                })
            
            # VÃ©rification montant suspect
            if 'montant_ttc' in doc_info:
                if doc_info['montant_ttc'] > 50000:
                    issues.append({
                        'severity': 'MEDIUM',
                        'issue': f"Montant Ã©levÃ© : {doc_info['montant_ttc']:,.2f} â‚¬ (>50k)"
                    })
                
                # Montant arrondi suspect
                if doc_info['montant_ttc'] % 1000 == 0:
                    issues.append({
                        'severity': 'LOW',
                        'issue': 'Montant arrondi Ã  1000â‚¬ (vÃ©rifier authenticitÃ©)'
                    })
        
        return issues
    
    def review_document(self, pdf_path):
        """
        Pipeline complet de revue
        """
        print(f"\n{'='*70}")
        print(f"REVUE AUTOMATISÃ‰E - {pdf_path}")
        print(f"{'='*70}")
        
        # 1. Extraction texte
        print(f"\n1ï¸âƒ£  Extraction du texte...")
        text = self.extract_text_from_pdf(pdf_path)
        
        if not text:
            print("   âŒ Impossible d'extraire le texte")
            return None
        
        print(f"   âœ… {len(text)} caractÃ¨res extraits")
        
        # 2. Classification
        print(f"\n2ï¸âƒ£  Classification du document...")
        classification = self.classify_document(text)
        doc_type = classification['category']
        confidence = classification['confidence']
        
        print(f"   Type dÃ©tectÃ© : {doc_type} (confiance: {confidence:.2%})")
        
        # 3. Extraction informations
        print(f"\n3ï¸âƒ£  Extraction des informations clÃ©s...")
        doc_info = self.extract_key_info(text, doc_type)
        
        for key, value in doc_info.items():
            print(f"   â€¢ {key:20s}: {value}")
        
        # 4. VÃ©rification conformitÃ©
        print(f"\n4ï¸âƒ£  VÃ©rification de conformitÃ©...")
        issues = self.check_compliance(doc_info, doc_type)
        
        if not issues:
            print(f"   âœ… Aucun problÃ¨me dÃ©tectÃ©")
        else:
            print(f"   âš ï¸  {len(issues)} problÃ¨me(s) dÃ©tectÃ©(s):")
            for issue in issues:
                severity_icon = {
                    'HIGH': 'ğŸ”´',
                    'MEDIUM': 'ğŸŸ ',
                    'LOW': 'ğŸŸ¡'
                }
                print(f"   {severity_icon[issue['severity']]} {issue['issue']}")
        
        return {
            'doc_type': doc_type,
            'confidence': confidence,
            'extracted_info': doc_info,
            'compliance_issues': issues
        }

# Utilisation
reviewer = DocumentReviewAutomation()
# result = reviewer.review_document('facture_001.pdf')
```

---

## 9.4 Scoring de Risque Fournisseurs

### 9.4.1 Ã‰valuation Multi-CritÃ¨res

```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier

class SupplierRiskScoring:
    """
    Scoring de risque fournisseurs basÃ© sur multiple critÃ¨res
    """
    
    def __init__(self):
        self.model = None
        self.risk_categories = {
            'financial': 0.30,      # 30% du score
            'operational': 0.25,    # 25%
            'compliance': 0.25,     # 25%
            'reputation': 0.20      # 20%
        }
    
    def calculate_financial_risk(self, supplier_data):
        """
        Risque financier : solvabilitÃ©, santÃ© financiÃ¨re
        """
        score = 0
        
        # Ratio dette/equity
        if supplier_data.get('debt_to_equity', 0) > 2:
            score += 30
        elif supplier_data.get('debt_to_equity', 0) > 1:
            score += 15
        
        # LiquiditÃ©
        current_ratio = supplier_data.get('current_ratio', 1)
        if current_ratio < 1:
            score += 25
        elif current_ratio < 1.5:
            score += 10
        
        # RentabilitÃ©
        if supplier_data.get('profit_margin', 0) < 0:
            score += 30  # Pertes
        elif supplier_data.get('profit_margin', 0) < 0.05:
            score += 15  # Faible marge
        
        # Historique paiements
        late_payments = supplier_data.get('late_payment_incidents', 0)
        if late_payments > 5:
            score += 20
        elif late_payments > 2:
            score += 10
        
        return min(score, 100)  # Cap Ã  100
    
    def calculate_operational_risk(self, supplier_data):
        """
        Risque opÃ©rationnel : capacitÃ© de livraison, qualitÃ©
        """
        score = 0
        
        # Taux de retard livraison
        delivery_rate = supplier_data.get('on_time_delivery_rate', 100)
        if delivery_rate < 80:
            score += 30
        elif delivery_rate < 90:
            score += 15
        
        # Taux de dÃ©fauts qualitÃ©
        defect_rate = supplier_data.get('defect_rate', 0)
        if defect_rate > 5:
            score += 25
        elif defect_rate > 2:
            score += 10
        
        # DÃ©pendance (% CA du fournisseur reprÃ©sentÃ© par notre entreprise)
        dependency = supplier_data.get('revenue_dependency_pct', 0)
        if dependency > 50:
            score += 20  # Trop dÃ©pendant
        elif dependency > 30:
            score += 10
        
        # CapacitÃ© de production
        utilization = supplier_data.get('capacity_utilization', 50)
        if utilization > 95:
            score += 15  # Surutilisation = risque
        
        return min(score, 100)
    
    def calculate_compliance_risk(self, supplier_data):
        """
        Risque conformitÃ© : certifications, rÃ©glementation
        """
        score = 0
        
        # Certifications manquantes
        required_certs = ['ISO9001', 'ISO14001']
        supplier_certs = supplier_data.get('certifications', [])
        
        missing_certs = set(required_certs) - set(supplier_certs)
        score += len(missing_certs) * 20
        
        # Incidents de non-conformitÃ©
        incidents = supplier_data.get('compliance_incidents', 0)
        if incidents > 3:
            score += 40
        elif incidents > 1:
            score += 20
        
        # Audit score
        audit_score = supplier_data.get('last_audit_score', 100)
        if audit_score < 60:
            score += 30
        elif audit_score < 80:
            score += 15
        
        # Pays Ã  risque
        if supplier_data.get('country') in ['Country1', 'Country2']:  # High-risk countries
            score += 25
        
        return min(score, 100)
    
    def calculate_reputation_risk(self, supplier_data):
        """
        Risque rÃ©putation : mÃ©dias, litiges, ESG
        """
        score = 0
        
        # Litiges en cours
        lawsuits = supplier_data.get('active_lawsuits', 0)
        if lawsuits > 2:
            score += 30
        elif lawsuits > 0:
            score += 15
        
        # Score ESG
        esg_score = supplier_data.get('esg_score', 50)
        if esg_score < 30:
            score += 25
        elif esg_score < 50:
            score += 10
        
        # Couverture mÃ©diatique nÃ©gative
        negative_news = supplier_data.get('negative_news_count_12m', 0)
        if negative_news > 5:
            score += 20
        elif negative_news > 2:
            score += 10
        
        return min(score, 100)
    
    def calculate_overall_risk(self, supplier_data):
        """
        Score de risque global pondÃ©rÃ©
        """
        risks = {
            'financial': self.calculate_financial_risk(supplier_data),
            'operational': self.calculate_operational_risk(supplier_data),
            'compliance': self.calculate_compliance_risk(supplier_data),
            'reputation': self.calculate_reputation_risk(supplier_data)
        }
        
        # Score pondÃ©rÃ©
        overall_score = sum(
            risks[category] * weight 
            for category, weight in self.risk_categories.items()
        )
        
        # Classification
        if overall_score < 30:
            risk_level = 'LOW'
            recommendation = 'Approved - Standard monitoring'
        elif overall_score < 60:
            risk_level = 'MEDIUM'
            recommendation = 'Approved with conditions - Enhanced monitoring'
        elif overall_score < 80:
            risk_level = 'HIGH'
            recommendation = 'Conditional approval - Mitigation plan required'
        else:
            risk_level = 'CRITICAL'
            recommendation = 'Not recommended - Find alternative supplier'
        
        return {
            'overall_score': overall_score,
            'risk_level': risk_level,
            'recommendation': recommendation,
            'category_scores': risks
        }
    
    def generate_report(self, supplier_name, supplier_data):
        """
        GÃ©nÃ©rer rapport d'Ã©valuation
        """
        result = self.calculate_overall_risk(supplier_data)
        
        print(f"\n{'='*70}")
        print(f"RAPPORT D'Ã‰VALUATION FOURNISSEUR - {supplier_name}")
        print(f"{'='*70}")
        
        print(f"\nğŸ“Š SCORE DE RISQUE GLOBAL : {result['overall_score']:.1f}/100")
        print(f"ğŸ¯ NIVEAU DE RISQUE      : {result['risk_level']}")
        print(f"ğŸ’¡ RECOMMANDATION        : {result['recommendation']}")
        
        print(f"\nğŸ“ˆ DÃ‰TAIL PAR CATÃ‰GORIE :\n")
        for category, score in result['category_scores'].items():
            weight = self.risk_categories[category]
            weighted_contribution = score * weight
            
            status = 'âœ…' if score < 30 else 'ğŸŸ ' if score < 60 else 'ğŸ”´'
            print(f"   {status} {category.upper():15s}: {score:>5.1f}/100 (poids: {weight:.0%}) â†’ Contribution: {weighted_contribution:.1f}")
        
        print(f"\nğŸ” ACTIONS RECOMMANDÃ‰ES :")
        
        actions = []
        if result['category_scores']['financial'] > 60:
            actions.append("â€¢ Demander garanties financiÃ¨res supplÃ©mentaires")
            actions.append("â€¢ Ã‰tablir plan de paiement sÃ©curisÃ©")
        
        if result['category_scores']['operational'] > 60:
            actions.append("â€¢ Auditer capacitÃ© de production")
            actions.append("â€¢ Identifier fournisseurs alternatifs (backup)")
        
        if result['category_scores']['compliance'] > 60:
            actions.append("â€¢ Exiger mise en conformitÃ© sous 90 jours")
            actions.append("â€¢ Planifier audit sur site")
        
        if result['category_scores']['reputation'] > 60:
            actions.append("â€¢ Due diligence approfondie")
            actions.append("â€¢ Clauses de protection rÃ©putation dans contrat")
        
        if not actions:
            print("   âœ… Aucune action particuliÃ¨re requise")
        else:
            for action in actions:
                print(f"   {action}")
        
        return result

# Exemple d'utilisation
scorer = SupplierRiskScoring()

supplier_example = {
    'name': 'TechSupply Corp',
    # Financier
    'debt_to_equity': 1.8,
    'current_ratio': 1.2,
    'profit_margin': 0.08,
    'late_payment_incidents': 1,
    # OpÃ©rationnel
    'on_time_delivery_rate': 92,
    'defect_rate': 1.5,
    'revenue_dependency_pct': 25,
    'capacity_utilization': 75,
    # ConformitÃ©
    'certifications': ['ISO9001'],
    'compliance_incidents': 0,
    'last_audit_score': 85,
    'country': 'FR',
    # RÃ©putation
    'active_lawsuits': 0,
    'esg_score': 65,
    'negative_news_count_12m': 1
}

report = scorer.generate_report('TechSupply Corp', supplier_example)
```

---

## 9.5 Cas RÃ©els d'ImplÃ©mentation

### 9.5.1 Deloitte - Audit Analytics Platform

**Solution** :
Plateforme d'audit augmentÃ©e analysant 100% des transactions vs Ã©chantillonnage traditionnel.

**Technologies** :
- NLP pour analyse documentaire
- Anomaly detection (Isolation Forest)
- Network analysis pour dÃ©tecter schÃ©mas de fraude
- Visualization interactive (Power BI + Python)

**RÃ©sultats** :
- **Couverture** : 100% vs 5-10% traditionnel
- **Temps d'audit** : -40%
- **Anomalies dÃ©tectÃ©es** : +200%
- **Faux positifs** : -60% (ML apprend des retours auditeurs)

### 9.5.2 EY - Canvas Tax Analyzer

**ProblÃ©matique** :
VÃ©rifier conformitÃ© fiscale sur millions de transactions.

**Solution IA** :
- Extraction automatique donnÃ©es fiscales (OCR + NLP)
- Validation rÃ¨gles fiscales par juridiction
- Calcul automatique risques/opportunitÃ©s
- GÃ©nÃ©ration rapports conformitÃ©

**Impact** :
- **Temps de revue** : Semaines â†’ Heures
- **PrÃ©cision** : +35%
- **Ã‰conomies fiscales identifiÃ©es** : +$50M pour clients


---

<a name="chapitre-7-marketing"></a>
# Chapitre 7 â€” IA en Marketing

## 7.1 Introduction Ã  l'IA Marketing

### 7.1.1 Transformation du Marketing par l'IA

Le marketing connaÃ®t une rÃ©volution grÃ¢ce Ã  l'IA, passant d'une approche de masse Ã  une hyper-personnalisation Ã  grande Ã©chelle.

**Ã‰volution du Marketing** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MARKETING TRADITIONNEL vs IA MARKETING            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  AVANT (Marketing de Masse)        MAINTENANT (IA)        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚  â€¢ Segmentation large              â€¢ Segments de 1        â”‚
â”‚  â€¢ Message unique                  â€¢ PersonnalisÃ© 1:1     â”‚
â”‚  â€¢ Campagnes statiques             â€¢ Optimisation temps   â”‚
â”‚  â€¢ DÃ©cisions intuition             â€¢ DÃ©cisions data       â”‚
â”‚  â€¢ ROI difficile Ã  mesurer         â€¢ Attribution prÃ©cise  â”‚
â”‚  â€¢ RÃ©activitÃ© lente                â€¢ Temps rÃ©el          â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Domaines d'Application** :
1. **Segmentation Client** : Clustering avancÃ©, micro-segments
2. **PrÃ©diction du Churn** : Identifier clients Ã  risque
3. **Recommandation** : Produits, contenu, next-best-action
4. **Optimisation Campagnes** : Attribution, budget allocation
5. **GÃ©nÃ©ration de Contenu** : Textes, images, vidÃ©os
6. **Pricing Dynamique** : Prix optimisÃ©s en temps rÃ©el
7. **Analyse Sentiment** : Social listening, avis clients

---

## 7.2 Segmentation Client AvancÃ©e

### 7.2.1 Au-delÃ  de la Segmentation Traditionnelle

**Segmentation Traditionnelle** (limitÃ©e) :
- DÃ©mographique : Ã¢ge, sexe, revenu
- GÃ©ographique : rÃ©gion, ville
- Psychographique : style de vie, valeurs

**Segmentation IA** (puissante) :
- Comportementale : navigation, achats, engagement
- PrÃ©dictive : propension, valeur vie client (CLV)
- Temporelle : moments de vie, saisonnalitÃ©
- Multi-dimensionnelle : 100+ variables

### 7.2.2 ImplÃ©mentation RFM + Machine Learning

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

class CustomerSegmentation:
    """
    Segmentation client avancÃ©e : RFM + Comportemental + ML
    """
    
    def __init__(self, n_segments=5):
        self.n_segments = n_segments
        self.scaler = StandardScaler()
        self.kmeans = KMeans(n_clusters=n_segments, random_state=42, n_init=10)
        self.pca = PCA(n_components=2)
        
    def calculate_rfm(self, transactions_df, reference_date=None):
        """
        Calcul des scores RFM (Recency, Frequency, Monetary)
        """
        if reference_date is None:
            reference_date = transactions_df['date'].max()
        
        rfm = transactions_df.groupby('customer_id').agg({
            'date': lambda x: (reference_date - x.max()).days,  # Recency
            'order_id': 'count',                                 # Frequency
            'amount': 'sum'                                      # Monetary
        })
        
        rfm.columns = ['recency', 'frequency', 'monetary']
        
        # Scores RFM (1-5, 5 = meilleur)
        rfm['R_score'] = pd.qcut(rfm['recency'], q=5, labels=[5,4,3,2,1], duplicates='drop')
        rfm['F_score'] = pd.qcut(rfm['frequency'], q=5, labels=[1,2,3,4,5], duplicates='drop')
        rfm['M_score'] = pd.qcut(rfm['monetary'], q=5, labels=[1,2,3,4,5], duplicates='drop')
        
        # Score RFM combinÃ©
        rfm['RFM_score'] = (
            rfm['R_score'].astype(int) * 100 + 
            rfm['F_score'].astype(int) * 10 + 
            rfm['M_score'].astype(int)
        )
        
        return rfm
    
    def calculate_behavioral_features(self, transactions_df, products_df=None):
        """
        Features comportementales avancÃ©es
        """
        features = {}
        
        # Groupe par client
        grouped = transactions_df.groupby('customer_id')
        
        # FRÃ‰QUENCE D'ACHAT
        features['avg_days_between_orders'] = grouped['date'].apply(
            lambda x: x.diff().dt.days.mean() if len(x) > 1 else 999
        )
        
        # PANIER MOYEN
        features['avg_basket_size'] = grouped['amount'].mean()
        features['std_basket_size'] = grouped['amount'].std().fillna(0)
        
        # DIVERSITÃ‰ PRODUITS
        if 'product_id' in transactions_df.columns:
            features['num_unique_products'] = grouped['product_id'].nunique()
            features['product_diversity_ratio'] = (
                features['num_unique_products'] / grouped.size()
            )
        
        # CATÃ‰GORIES PRÃ‰FÃ‰RÃ‰ES
        if 'category' in transactions_df.columns:
            features['num_categories'] = grouped['category'].nunique()
            
            # CatÃ©gorie dominante (mode)
            features['dominant_category'] = grouped['category'].apply(
                lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Unknown'
            )
        
        # CANAL D'ACHAT
        if 'channel' in transactions_df.columns:
            features['pct_online'] = grouped.apply(
                lambda x: (x['channel'] == 'online').sum() / len(x)
            )
        
        # TEMPORALITÃ‰
        transactions_df['hour'] = pd.to_datetime(transactions_df['date']).dt.hour
        transactions_df['day_of_week'] = pd.to_datetime(transactions_df['date']).dt.dayofweek
        
        features['preferred_hour'] = grouped['hour'].apply(
            lambda x: x.mode()[0] if len(x.mode()) > 0 else 12
        )
        features['pct_weekend_orders'] = grouped.apply(
            lambda x: (x['day_of_week'] >= 5).sum() / len(x)
        )
        
        # PROMOTIONS
        if 'used_promo' in transactions_df.columns:
            features['promo_sensitivity'] = grouped.apply(
                lambda x: (x['used_promo'] == True).sum() / len(x)
            )
        
        # RETOURS
        if 'returned' in transactions_df.columns:
            features['return_rate'] = grouped.apply(
                lambda x: (x['returned'] == True).sum() / len(x)
            )
        
        # VALEUR VIE CLIENT (CLV simplifiÃ©)
        features['total_spent'] = grouped['amount'].sum()
        features['estimated_clv'] = (
            features['avg_basket_size'] * 
            (365 / features['avg_days_between_orders'].replace([np.inf, 0], 999))
        )
        
        # Convertir en DataFrame
        features_df = pd.DataFrame(features)
        
        return features_df
    
    def segment_customers(self, rfm_df, behavioral_df):
        """
        Segmentation ML (K-Means)
        """
        # Combiner RFM et comportemental
        features_df = rfm_df[['recency', 'frequency', 'monetary']].join(
            behavioral_df, how='inner'
        )
        
        # GÃ©rer valeurs manquantes
        features_df = features_df.fillna(features_df.median())
        
        # SÃ©lectionner features numÃ©riques
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        X = features_df[numeric_cols]
        
        # Standardisation (crucial pour K-Means)
        X_scaled = self.scaler.fit_transform(X)
        
        # Clustering
        clusters = self.kmeans.fit_predict(X_scaled)
        features_df['segment'] = clusters
        
        # PCA pour visualisation
        X_pca = self.pca.fit_transform(X_scaled)
        features_df['pca1'] = X_pca[:, 0]
        features_df['pca2'] = X_pca[:, 1]
        
        return features_df
    
    def profile_segments(self, segmented_df):
        """
        Profiler chaque segment
        """
        profiles = []
        
        for segment_id in range(self.n_segments):
            segment_data = segmented_df[segmented_df['segment'] == segment_id]
            
            profile = {
                'segment_id': segment_id,
                'size': len(segment_data),
                'size_pct': len(segment_data) / len(segmented_df) * 100,
                
                # RFM
                'avg_recency': segment_data['recency'].mean(),
                'avg_frequency': segment_data['frequency'].mean(),
                'avg_monetary': segment_data['monetary'].mean(),
                
                # CLV
                'avg_clv': segment_data['estimated_clv'].mean(),
                'total_value': segment_data['total_spent'].sum(),
                
                # Comportement
                'avg_basket': segment_data['avg_basket_size'].mean(),
                'avg_days_between': segment_data['avg_days_between_orders'].mean(),
            }
            
            # Ajouter features spÃ©cifiques si disponibles
            if 'promo_sensitivity' in segment_data.columns:
                profile['promo_sensitivity'] = segment_data['promo_sensitivity'].mean()
            
            if 'return_rate' in segment_data.columns:
                profile['return_rate'] = segment_data['return_rate'].mean()
            
            if 'pct_online' in segment_data.columns:
                profile['pct_online'] = segment_data['pct_online'].mean()
            
            profiles.append(profile)
        
        profiles_df = pd.DataFrame(profiles)
        
        return profiles_df
    
    def name_segments(self, profiles_df):
        """
        Nommer les segments de maniÃ¨re descriptive
        """
        profiles_df['segment_name'] = 'Unknown'
        
        for idx, row in profiles_df.iterrows():
            # Champions : Recency faible, Frequency Ã©levÃ©e, Monetary Ã©levÃ©
            if (row['avg_recency'] < 30 and 
                row['avg_frequency'] > profiles_df['avg_frequency'].median() and
                row['avg_monetary'] > profiles_df['avg_monetary'].median()):
                profiles_df.at[idx, 'segment_name'] = 'ğŸ† Champions'
            
            # Loyal : Frequency Ã©levÃ©e mais Monetary moyen
            elif (row['avg_frequency'] > profiles_df['avg_frequency'].median() and
                  row['avg_monetary'] <= profiles_df['avg_monetary'].median()):
                profiles_df.at[idx, 'segment_name'] = 'ğŸ’™ Loyaux'
            
            # Big Spenders : Monetary Ã©levÃ© mais Frequency faible
            elif (row['avg_monetary'] > profiles_df['avg_monetary'].median() and
                  row['avg_frequency'] <= profiles_df['avg_frequency'].median()):
                profiles_df.at[idx, 'segment_name'] = 'ğŸ’° Gros DÃ©pensiers'
            
            # At Risk : Recency Ã©levÃ©e mais bon historique
            elif (row['avg_recency'] > 90 and
                  row['avg_monetary'] > profiles_df['avg_monetary'].quantile(0.3)):
                profiles_df.at[idx, 'segment_name'] = 'âš ï¸  Ã€ Risque'
            
            # Lost : Recency trÃ¨s Ã©levÃ©e
            elif row['avg_recency'] > 180:
                profiles_df.at[idx, 'segment_name'] = 'ğŸ˜¢ Perdus'
            
            # New/Promising : Recency faible mais peu de transactions
            elif (row['avg_recency'] < 30 and
                  row['avg_frequency'] <= 2):
                profiles_df.at[idx, 'segment_name'] = 'ğŸŒ± Nouveaux/Prometteurs'
            
            # Need Attention : Autres
            else:
                profiles_df.at[idx, 'segment_name'] = 'ğŸ”” NÃ©cessitent Attention'
        
        return profiles_df
    
    def recommend_actions(self, profiles_df):
        """
        Recommander actions marketing par segment
        """
        actions = {
            'ğŸ† Champions': [
                "Programme VIP exclusif",
                "Early access nouveaux produits",
                "Ambassadeurs de marque (UGC, referral)",
                "Rewards programme premium"
            ],
            'ğŸ’™ Loyaux': [
                "Programme de fidÃ©litÃ© renforcÃ©",
                "Upsell produits premium",
                "Cross-sell catÃ©gories adjacentes",
                "Satisfaction surveys pour amÃ©lioration"
            ],
            'ğŸ’° Gros DÃ©pensiers': [
                "Concierge service personnalisÃ©",
                "Offres exclusives haut de gamme",
                "Ã‰vÃ©nements VIP",
                "Programme de rÃ©tention avec incentives"
            ],
            'âš ï¸  Ã€ Risque': [
                "Campagne de rÃ©activation urgente",
                "Offre spÃ©ciale 'We miss you' (20-30% off)",
                "EnquÃªte satisfaction + service recovery",
                "Rappel avantages programme fidÃ©litÃ©"
            ],
            'ğŸ˜¢ Perdus': [
                "Campagne win-back agressive (40-50% off)",
                "EnquÃªte dÃ©part (pourquoi nous ont quittÃ©)",
                "Nouvelle proposition de valeur",
                "Si pas de rÃ©ponse â†’ Retirer de la liste active"
            ],
            'ğŸŒ± Nouveaux/Prometteurs': [
                "Onboarding optimisÃ©",
                "2Ã¨me achat incentivÃ© (offre bienvenue)",
                "Tutoriels produits",
                "Programme de parrainage"
            ],
            'ğŸ”” NÃ©cessitent Attention': [
                "Campagne d'engagement",
                "Contenu Ã©ducatif",
                "Promotions ciblÃ©es",
                "A/B testing diffÃ©rentes approches"
            ]
        }
        
        profiles_df['recommended_actions'] = profiles_df['segment_name'].map(
            lambda x: actions.get(x, ["StratÃ©gie Ã  dÃ©finir"])
        )
        
        return profiles_df
    
    def visualize_segments(self, segmented_df, profiles_df):
        """
        Visualisation des segments
        """
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Scatter PCA
        scatter = axes[0, 0].scatter(
            segmented_df['pca1'], 
            segmented_df['pca2'],
            c=segmented_df['segment'],
            cmap='viridis',
            alpha=0.6
        )
        axes[0, 0].set_xlabel('PCA Component 1')
        axes[0, 0].set_ylabel('PCA Component 2')
        axes[0, 0].set_title('Segmentation Clients (PCA)')
        plt.colorbar(scatter, ax=axes[0, 0])
        
        # 2. Taille des segments
        profiles_df.plot.bar(
            x='segment_name',
            y='size',
            ax=axes[0, 1],
            legend=False,
            color='steelblue'
        )
        axes[0, 1].set_title('Taille des Segments')
        axes[0, 1].set_xlabel('Segment')
        axes[0, 1].set_ylabel('Nombre de Clients')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. CLV par segment
        profiles_df.plot.bar(
            x='segment_name',
            y='avg_clv',
            ax=axes[1, 0],
            legend=False,
            color='green'
        )
        axes[1, 0].set_title('CLV Moyen par Segment')
        axes[1, 0].set_xlabel('Segment')
        axes[1, 0].set_ylabel('CLV (â‚¬)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. RFM Heatmap
        rfm_data = profiles_df[['segment_name', 'avg_recency', 'avg_frequency', 'avg_monetary']].set_index('segment_name')
        sns.heatmap(
            rfm_data.T,
            annot=True,
            fmt='.0f',
            cmap='RdYlGn_r',
            ax=axes[1, 1]
        )
        axes[1, 1].set_title('RFM Heatmap par Segment')
        
        plt.tight_layout()
        # plt.savefig('customer_segmentation.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_report(self, profiles_df):
        """
        Rapport exÃ©cutif de segmentation
        """
        print("\n" + "="*80)
        print("RAPPORT DE SEGMENTATION CLIENT")
        print("="*80)
        
        print(f"\nğŸ“Š VUE D'ENSEMBLE")
        print(f"   Nombre de segments : {len(profiles_df)}")
        print(f"   Total clients      : {profiles_df['size'].sum():,}")
        
        print(f"\nğŸ’° VALEUR TOTALE")
        print(f"   Revenue total      : {profiles_df['total_value'].sum():,.0f} â‚¬")
        print(f"   CLV moyen global   : {profiles_df['avg_clv'].mean():,.0f} â‚¬")
        
        print(f"\nğŸ¯ PROFIL DES SEGMENTS\n")
        
        # Trier par CLV dÃ©croissant
        profiles_sorted = profiles_df.sort_values('avg_clv', ascending=False)
        
        for idx, row in profiles_sorted.iterrows():
            print(f"{row['segment_name']}")
            print(f"{'â”€'*70}")
            print(f"  ğŸ“ˆ Taille        : {row['size']:,} clients ({row['size_pct']:.1f}%)")
            print(f"  ğŸ’µ Valeur totale : {row['total_value']:,.0f} â‚¬")
            print(f"  ğŸ’ CLV moyen     : {row['avg_clv']:,.0f} â‚¬")
            print(f"  ğŸ”„ RÃ©cence       : {row['avg_recency']:.0f} jours")
            print(f"  ğŸ›’ FrÃ©quence     : {row['avg_frequency']:.1f} commandes")
            print(f"  ğŸ’° Panier moyen  : {row['avg_basket']:.0f} â‚¬")
            
            if 'promo_sensitivity' in row:
                print(f"  ğŸ·ï¸  Promo-sensible: {row['promo_sensitivity']:.1%}")
            
            print(f"\n  âœ… ACTIONS RECOMMANDÃ‰ES :")
            for action in row['recommended_actions']:
                print(f"     â€¢ {action}")
            print()

# UTILISATION COMPLÃˆTE
# =====================

# Charger donnÃ©es
transactions = pd.read_csv('transactions.csv')
# transactions columns: customer_id, date, order_id, amount, product_id, category, channel, used_promo, returned

# Initialiser
segmenter = CustomerSegmentation(n_segments=5)

# 1. Calculer RFM
rfm = segmenter.calculate_rfm(transactions)

# 2. Calculer features comportementales
behavioral = segmenter.calculate_behavioral_features(transactions)

# 3. Segmentation
segmented = segmenter.segment_customers(rfm, behavioral)

# 4. Profiler segments
profiles = segmenter.profile_segments(segmented)

# 5. Nommer segments
profiles = segmenter.name_segments(profiles)

# 6. Recommander actions
profiles = segmenter.recommend_actions(profiles)

# 7. Visualiser
segmenter.visualize_segments(segmented, profiles)

# 8. Rapport
segmenter.generate_report(profiles)

# 9. Exporter rÃ©sultats
segmented.to_csv('customers_segmented.csv', index=True)
profiles.to_csv('segments_profiles.csv', index=False)
```

---

## 7.3 PrÃ©diction du Churn (Attrition Client)

### 7.3.1 DÃ©finition et Enjeux

**Churn** = Taux d'attrition = % de clients qui cessent d'utiliser un produit/service

**Enjeux Business** :
- CoÃ»t acquisition client : 5-25Ã— plus cher que rÃ©tention
- Impact revenue : 5% rÃ©duction churn = 25-95% augmentation profits
- LTV (Lifetime Value) : Churn tue la valeur long terme

### 7.3.2 ModÃ¨le PrÃ©dictif de Churn

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import shap

class ChurnPredictionModel:
    """
    ModÃ¨le de prÃ©diction du churn client
    """
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = None
        self.shap_explainer = None
        
    def engineer_churn_features(self, customers_df, transactions_df, support_df=None):
        """
        Feature engineering pour prÃ©diction churn
        """
        features = pd.DataFrame(index=customers_df['customer_id'])
        
        # FEATURES DÃ‰MOGRAPHIQUES
        features['age'] = customers_df['age']
        features['tenure_months'] = (
            (pd.to_datetime('today') - pd.to_datetime(customers_df['signup_date']))
            .dt.days / 30
        )
        
        # FEATURES TRANSACTIONNELLES (30 derniers jours)
        recent_date = transactions_df['date'].max() - timedelta(days=30)
        recent_tx = transactions_df[transactions_df['date'] >= recent_date]
        
        tx_grouped = recent_tx.groupby('customer_id')
        features['num_transactions_30d'] = tx_grouped.size().reindex(features.index, fill_value=0)
        features['total_spent_30d'] = tx_grouped['amount'].sum().reindex(features.index, fill_value=0)
        features['avg_basket_30d'] = tx_grouped['amount'].mean().reindex(features.index, fill_value=0)
        
        # FEATURES TEMPORELLES (tendances)
        # Comparer 30 derniers jours vs 30-60 jours
        prev_period = transactions_df[
            (transactions_df['date'] >= recent_date - timedelta(days=30)) &
            (transactions_df['date'] < recent_date)
        ]
        
        prev_grouped = prev_period.groupby('customer_id')
        features['num_transactions_prev30d'] = prev_grouped.size().reindex(features.index, fill_value=0)
        features['total_spent_prev30d'] = prev_grouped['amount'].sum().reindex(features.index, fill_value=0)
        
        # Tendance (variation %)
        features['transaction_trend'] = (
            (features['num_transactions_30d'] - features['num_transactions_prev30d']) /
            (features['num_transactions_prev30d'] + 1) * 100
        )
        features['spending_trend'] = (
            (features['total_spent_30d'] - features['total_spent_prev30d']) /
            (features['total_spent_prev30d'] + 1) * 100
        )
        
        # FEATURES ENGAGEMENT
        # Jours depuis derniÃ¨re transaction
        last_tx = transactions_df.groupby('customer_id')['date'].max()
        features['days_since_last_purchase'] = (
            (transactions_df['date'].max() - last_tx).dt.days
        ).reindex(features.index, fill_value=999)
        
        # RÃ©gularitÃ© d'achat (Ã©cart-type entre transactions)
        tx_dates = transactions_df.groupby('customer_id')['date'].apply(
            lambda x: x.diff().dt.days.std() if len(x) > 1 else 0
        )
        features['purchase_regularity_std'] = tx_dates.reindex(features.index, fill_value=999)
        
        # FEATURES SUPPORT CLIENT
        if support_df is not None:
            support_grouped = support_df.groupby('customer_id')
            
            features['num_support_tickets_30d'] = (
                support_grouped['ticket_id'].count().reindex(features.index, fill_value=0)
            )
            features['avg_resolution_time'] = (
                support_grouped['resolution_hours'].mean().reindex(features.index, fill_value=0)
            )
            features['pct_unresolved'] = (
                support_grouped.apply(lambda x: (x['status'] == 'open').sum() / len(x))
                .reindex(features.index, fill_value=0)
            )
            features['negative_satisfaction'] = (
                support_grouped.apply(lambda x: (x['satisfaction'] <= 2).sum())
                .reindex(features.index, fill_value=0)
            )
        
        # FEATURES PRODUIT/SERVICE
        if 'subscription_tier' in customers_df.columns:
            features['subscription_tier'] = customers_df['subscription_tier'].map({
                'basic': 1, 'standard': 2, 'premium': 3
            })
        
        if 'num_features_used' in customers_df.columns:
            features['feature_adoption_rate'] = (
                customers_df['num_features_used'] / customers_df['total_features']
            )
        
        # FEATURES COMPORTEMENTALES
        if 'login_count_30d' in customers_df.columns:
            features['login_frequency'] = customers_df['login_count_30d']
            features['days_since_last_login'] = customers_df['days_since_last_login']
        
        # FLAGS DE RISQUE
        features['high_risk_flag'] = (
            (features['days_since_last_purchase'] > 60) |
            (features['transaction_trend'] < -50) |
            (features['spending_trend'] < -50)
        ).astype(int)
        
        features['very_high_risk_flag'] = (
            (features['days_since_last_purchase'] > 90) &
            (features['num_transactions_30d'] == 0)
        ).astype(int)
        
        return features
    
    def train_model(self, X_train, y_train, X_val, y_val):
        """
        EntraÃ®ner modÃ¨le de prÃ©diction
        """
        # GÃ©rer dÃ©sÃ©quilibre de classes (gÃ©nÃ©ralement 5-20% churn)
        churn_rate = y_train.mean()
        scale_pos_weight = (1 - churn_rate) / churn_rate
        
        # XGBoost avec gestion dÃ©sÃ©quilibre
        self.model = XGBClassifier(
            n_estimators=300,
            max_depth=5,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=scale_pos_weight,
            eval_metric='auc',
            random_state=42,
            early_stopping_rounds=30
        )
        
        # Standardisation
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        # EntraÃ®nement
        self.model.fit(
            X_train_scaled, y_train,
            eval_set=[(X_val_scaled, y_val)],
            verbose=False
        )
        
        # SHAP pour explicabilitÃ©
        self.shap_explainer = shap.TreeExplainer(self.model)
        
        # Ã‰valuation
        y_pred_proba = self.model.predict_proba(X_val_scaled)[:, 1]
        auc = roc_auc_score(y_val, y_pred_proba)
        
        print(f"\n{'='*70}")
        print(f"PERFORMANCE MODÃˆLE CHURN")
        print(f"{'='*70}")
        print(f"AUC-ROC Validation : {auc:.4f}")
        
        # Matrice de confusion (seuil 0.5)
        y_pred_class = (y_pred_proba > 0.5).astype(int)
        print(f"\nClassification Report :")
        print(classification_report(y_val, y_pred_class, target_names=['Reste', 'Churn']))
        
        return auc
    
    def predict_churn_probability(self, X):
        """
        PrÃ©dire probabilitÃ© de churn
        """
        X_scaled = self.scaler.transform(X)
        churn_proba = self.model.predict_proba(X_scaled)[:, 1]
        return churn_proba
    
    def segment_by_churn_risk(self, customers_df, churn_probabilities):
        """
        Segmenter clients par niveau de risque
        """
        customers_df['churn_probability'] = churn_probabilities
        
        # Segments de risque
        customers_df['risk_segment'] = pd.cut(
            churn_probabilities,
            bins=[0, 0.2, 0.5, 0.7, 1.0],
            labels=['Faible', 'Moyen', 'Ã‰levÃ©', 'Critique']
        )
        
        return customers_df
    
    def recommend_retention_actions(self, customers_df):
        """
        Recommander actions de rÃ©tention personnalisÃ©es
        """
        actions = []
        
        for _, customer in customers_df.iterrows():
            customer_actions = {
                'customer_id': customer['customer_id'],
                'churn_risk': customer['risk_segment'],
                'actions': []
            }
            
            risk = customer['risk_segment']
            
            if risk == 'Critique':
                customer_actions['priority'] = 'URGENT'
                customer_actions['actions'] = [
                    "ğŸ“ Appel personnel Account Manager dans 24h",
                    "ğŸ’° Offre de rÃ©tention agressive (30-50% rÃ©duction)",
                    "ğŸ Bonus/CrÃ©dits immÃ©diats",
                    "ğŸ“Š EnquÃªte satisfaction + service recovery plan"
                ]
            
            elif risk == 'Ã‰levÃ©':
                customer_actions['priority'] = 'HIGH'
                customer_actions['actions'] = [
                    "ğŸ“§ Email personnalisÃ© du CSM",
                    "ğŸ’µ Offre de rÃ©tention (15-20% rÃ©duction)",
                    "ğŸ”„ Proposition upgrade/downgrade selon besoin",
                    "ğŸ“± Engagement proactif (check-in call)"
                ]
            
            elif risk == 'Moyen':
                customer_actions['priority'] = 'MEDIUM'
                customer_actions['actions'] = [
                    "ğŸ“¬ Campagne re-engagement automatisÃ©e",
                    "ğŸ¯ Offre ciblÃ©e selon historique",
                    "ğŸ“š Contenu Ã©ducatif (maximiser valeur produit)",
                    "â­ Programme fidÃ©litÃ© renforcÃ©"
                ]
            
            else:  # Faible
                customer_actions['priority'] = 'LOW'
                customer_actions['actions'] = [
                    "âœ… Monitoring passif",
                    "ğŸ’¬ NPS survey pour maintenir satisfaction",
                    "ğŸ”” Upsell opportuniste si pertinent"
                ]
            
            actions.append(customer_actions)
        
        return pd.DataFrame(actions)
    
    def calculate_roi(self, customers_df, actions_df):
        """
        Calculer ROI de la campagne de rÃ©tention
        """
        # HypothÃ¨ses
        avg_clv = customers_df['clv'].mean()
        retention_cost = {
            'Critique': 200,  # CoÃ»t intervention critique
            'Ã‰levÃ©': 100,
            'Moyen': 30,
            'Faible': 5
        }
        retention_success_rate = {
            'Critique': 0.40,  # 40% de succÃ¨s
            'Ã‰levÃ©': 0.60,
            'Moyen': 0.75,
            'Faible': 0.90
        }
        
        roi_analysis = []
        
        for risk_level in ['Critique', 'Ã‰levÃ©', 'Moyen', 'Faible']:
            segment = customers_df[customers_df['risk_segment'] == risk_level]
            n_customers = len(segment)
            
            if n_customers == 0:
                continue
            
            # CoÃ»ts
            total_cost = n_customers * retention_cost[risk_level]
            
            # BÃ©nÃ©fices (clients sauvÃ©s Ã— CLV)
            customers_saved = n_customers * retention_success_rate[risk_level]
            total_benefit = customers_saved * avg_clv
            
            # ROI
            net_benefit = total_benefit - total_cost
            roi_pct = (net_benefit / total_cost) * 100 if total_cost > 0 else 0
            
            roi_analysis.append({
                'risk_segment': risk_level,
                'num_customers': n_customers,
                'total_cost': total_cost,
                'customers_saved': customers_saved,
                'total_benefit': total_benefit,
                'net_benefit': net_benefit,
                'roi_pct': roi_pct
            })
        
        roi_df = pd.DataFrame(roi_analysis)
        
        print(f"\n{'='*70}")
        print(f"ANALYSE ROI CAMPAGNE RÃ‰TENTION")
        print(f"{'='*70}\n")
        
        for _, row in roi_df.iterrows():
            print(f"ğŸ“Š Segment : {row['risk_segment']}")
            print(f"   Clients ciblÃ©s    : {row['num_customers']:,}")
            print(f"   CoÃ»t campagne     : {row['total_cost']:,.0f} â‚¬")
            print(f"   Clients sauvÃ©s (est): {row['customers_saved']:.0f}")
            print(f"   BÃ©nÃ©fice total    : {row['total_benefit']:,.0f} â‚¬")
            print(f"   BÃ©nÃ©fice net      : {row['net_benefit']:,.0f} â‚¬")
            print(f"   ROI               : {row['roi_pct']:.0f}%")
            print()
        
        # Total
        print(f"ğŸ’° TOTAL CAMPAGNE")
        print(f"   CoÃ»t total        : {roi_df['total_cost'].sum():,.0f} â‚¬")
        print(f"   BÃ©nÃ©fice net total: {roi_df['net_benefit'].sum():,.0f} â‚¬")
        print(f"   ROI global        : {(roi_df['net_benefit'].sum() / roi_df['total_cost'].sum() * 100):.0f}%")
        
        return roi_df

# Utilisation
model = ChurnPredictionModel()

# Feature engineering
features = model.engineer_churn_features(customers, transactions, support_tickets)

# EntraÃ®nement
X_train, X_test, y_train, y_test = train_test_split(
    features, customers['churned'], 
    test_size=0.2, stratify=customers['churned'], 
    random_state=42
)

model.train_model(X_train, y_train, X_test, y_test)

# PrÃ©diction
churn_probas = model.predict_churn_probability(features)

# Segmentation
customers_segmented = model.segment_by_churn_risk(customers, churn_probas)

# Actions recommandÃ©es
actions = model.recommend_retention_actions(customers_segmented)

# ROI
roi = model.calculate_roi(customers_segmented, actions)
```

---

## 7.4 SystÃ¨me de Recommandation

### 7.4.1 Types de Recommandation

**1. Filtrage Collaboratif** :
- User-based : "Les utilisateurs comme vous ont aimÃ©..."
- Item-based : "Les clients qui ont achetÃ© X ont aussi achetÃ© Y"

**2. Filtrage par Contenu** :
- Recommander produits similaires Ã  ceux aimÃ©s
- BasÃ© sur attributs produits

**3. Hybride** :
- Combiner collaboratif + contenu
- Approche la plus performante

### 7.4.2 ImplÃ©mentation

```python
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD

class ProductRecommendationEngine:
    """
    SystÃ¨me de recommandation produits (Collaborative Filtering)
    """
    
    def __init__(self, n_recommendations=10):
        self.n_recommendations = n_recommendations
        self.user_item_matrix = None
        self.similarity_matrix = None
        self.svd_model = None
        
    def create_user_item_matrix(self, interactions_df):
        """
        CrÃ©er matrice utilisateur-produit
        """
        # Pivot : lignes=users, colonnes=products, valeurs=ratings/achats
        self.user_item_matrix = interactions_df.pivot_table(
            index='user_id',
            columns='product_id',
            values='rating',  # ou 'quantity' si pas de ratings
            fill_value=0
        )
        
        return self.user_item_matrix
    
    def item_based_collaborative_filtering(self):
        """
        Collaborative filtering item-based (produits similaires)
        """
        # Calculer similaritÃ© cosinus entre produits
        item_similarity = cosine_similarity(self.user_item_matrix.T)
        
        self.similarity_matrix = pd.DataFrame(
            item_similarity,
            index=self.user_item_matrix.columns,
            columns=self.user_item_matrix.columns
        )
        
        return self.similarity_matrix
    
    def recommend_for_user(self, user_id, exclude_purchased=True):
        """
        Recommander produits pour un utilisateur
        """
        if user_id not in self.user_item_matrix.index:
            return self.recommend_popular()  # Fallback
        
        # Produits dÃ©jÃ  achetÃ©s/notÃ©s par user
        user_ratings = self.user_item_matrix.loc[user_id]
        purchased_items = user_ratings[user_ratings > 0].index
        
        # Calculer scores de recommandation
        scores = {}
        
        for item in purchased_items:
            # SimilaritÃ©s de cet item avec tous les autres
            similar_items = self.similarity_matrix[item].sort_values(ascending=False)
            
            # PondÃ©rer par rating utilisateur
            user_rating = user_ratings[item]
            
            for similar_item, similarity in similar_items.items():
                if similar_item != item:  # Pas recommander mÃªme produit
                    if similar_item not in scores:
                        scores[similar_item] = 0
                    scores[similar_item] += similarity * user_rating
        
        # Trier par score
        recommendations = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        # Exclure dÃ©jÃ  achetÃ©s si demandÃ©
        if exclude_purchased:
            recommendations = [
                (item, score) for item, score in recommendations 
                if item not in purchased_items
            ]
        
        return recommendations[:self.n_recommendations]
    
    def recommend_popular(self, n=10):
        """
        Recommander produits populaires (fallback)
        """
        # Somme des ratings/achats par produit
        popularity = self.user_item_matrix.sum(axis=0).sort_values(ascending=False)
        return list(popularity.head(n).index)
    
    def matrix_factorization_svd(self, n_factors=50):
        """
        Matrix Factorization avec SVD (scalable pour grandes matrices)
        """
        # SVD
        self.svd_model = TruncatedSVD(n_components=n_factors, random_state=42)
        user_factors = self.svd_model.fit_transform(self.user_item_matrix)
        item_factors = self.svd_model.components_.T
        
        # Reconstruction ratings prÃ©dits
        predicted_ratings = np.dot(user_factors, item_factors.T)
        
        self.predicted_matrix = pd.DataFrame(
            predicted_ratings,
            index=self.user_item_matrix.index,
            columns=self.user_item_matrix.columns
        )
        
        return self.predicted_matrix
    
    def recommend_with_mf(self, user_id, exclude_purchased=True):
        """
        Recommander avec Matrix Factorization
        """
        if user_id not in self.predicted_matrix.index:
            return self.recommend_popular()
        
        # PrÃ©dictions pour cet utilisateur
        user_predictions = self.predicted_matrix.loc[user_id].sort_values(ascending=False)
        
        if exclude_purchased:
            # Exclure dÃ©jÃ  achetÃ©s
            purchased = self.user_item_matrix.loc[user_id]
            purchased_items = purchased[purchased > 0].index
            user_predictions = user_predictions.drop(purchased_items, errors='ignore')
        
        return list(user_predictions.head(self.n_recommendations).index)
    
    def evaluate_recommendations(self, test_interactions):
        """
        Ã‰valuer qualitÃ© des recommandations
        """
        precisions = []
        recalls = []
        
        for user_id in test_interactions['user_id'].unique():
            # Vraies interactions test
            true_items = set(
                test_interactions[test_interactions['user_id'] == user_id]['product_id']
            )
            
            # Recommandations
            recommended_items = set([
                item for item, score in self.recommend_for_user(user_id)
            ])
            
            if len(recommended_items) > 0:
                # Precision@K
                hits = len(true_items & recommended_items)
                precision = hits / len(recommended_items)
                precisions.append(precision)
                
                # Recall@K
                if len(true_items) > 0:
                    recall = hits / len(true_items)
                    recalls.append(recall)
        
        avg_precision = np.mean(precisions) if precisions else 0
        avg_recall = np.mean(recalls) if recalls else 0
        f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0
        
        print(f"\n{'='*70}")
        print(f"Ã‰VALUATION SYSTÃˆME DE RECOMMANDATION")
        print(f"{'='*70}")
        print(f"Precision@{self.n_recommendations} : {avg_precision:.3f}")
        print(f"Recall@{self.n_recommendations}    : {avg_recall:.3f}")
        print(f"F1-Score             : {f1:.3f}")
        
        return {'precision': avg_precision, 'recall': avg_recall, 'f1': f1}

# Utilisation
recommender = ProductRecommendationEngine(n_recommendations=10)

# CrÃ©er matrice user-item
interactions = pd.DataFrame({
    'user_id': [...],
    'product_id': [...],
    'rating': [...]  # ou 'quantity' pour implicit feedback
})

user_item_matrix = recommender.create_user_item_matrix(interactions)

# Method 1: Item-based CF
recommender.item_based_collaborative_filtering()
reco_user_123 = recommender.recommend_for_user(user_id=123)

# Method 2: Matrix Factorization
recommender.matrix_factorization_svd(n_factors=50)
reco_user_123_mf = recommender.recommend_with_mf(user_id=123)

# Ã‰valuation
metrics = recommender.evaluate_recommendations(test_interactions)
```

### 7.4.3 Cas RÃ©el : Spotify - Discover Weekly

**Challenge** :
Recommander 30 chansons personnalisÃ©es chaque lundi Ã  200M+ utilisateurs.

**Solution Multi-Algorithmes** :

1. **Collaborative Filtering** :
   - Matrice 200M users Ã— 50M chansons
   - Matrix Factorization (ALS - Alternating Least Squares)
   - Implicit feedback (Ã©coutes, skips, saves)

2. **NLP sur MÃ©tadonnÃ©es** :
   - Analyse paroles, descriptions artistes
   - Word2Vec sur playlists (chansons = mots)

3. **Audio Analysis** :
   - CNN sur spectrogrammes
   - Features : tempo, valence, acousticness, etc.

4. **Contextual Bandits** :
   - A/B testing en temps rÃ©el
   - Apprentissage des prÃ©fÃ©rences utilisateur

**RÃ©sultats** :
- **40%** des utilisateurs Ã©coutent Discover Weekly
- **5 milliards** de chansons streamÃ©es via Discover Weekly
- **Engagement** : +20% temps d'Ã©coute
- **DÃ©couvertes** : 90% des chansons recommandÃ©es sont nouvelles pour l'utilisateur

